{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run ./main.py\n",
    "\n",
    "# uncomment to run main.py to generate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>rsi_6</th>\n",
       "      <th>rsi_7</th>\n",
       "      <th>rsi_8</th>\n",
       "      <th>...</th>\n",
       "      <th>eom_18_EURUSD=X</th>\n",
       "      <th>eom_19_EURUSD=X</th>\n",
       "      <th>eom_20_EURUSD=X</th>\n",
       "      <th>eom_21_EURUSD=X</th>\n",
       "      <th>eom_22_EURUSD=X</th>\n",
       "      <th>eom_23_EURUSD=X</th>\n",
       "      <th>eom_24_EURUSD=X</th>\n",
       "      <th>eom_25_EURUSD=X</th>\n",
       "      <th>eom_26_EURUSD=X</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>810</td>\n",
       "      <td>2004-01-06</td>\n",
       "      <td>17.814430</td>\n",
       "      <td>17.833448</td>\n",
       "      <td>17.662278</td>\n",
       "      <td>17.776392</td>\n",
       "      <td>691100</td>\n",
       "      <td>91.334101</td>\n",
       "      <td>81.891705</td>\n",
       "      <td>82.829501</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>811</td>\n",
       "      <td>2004-01-07</td>\n",
       "      <td>17.681286</td>\n",
       "      <td>17.681286</td>\n",
       "      <td>17.484757</td>\n",
       "      <td>17.592531</td>\n",
       "      <td>1676800</td>\n",
       "      <td>72.556789</td>\n",
       "      <td>77.477096</td>\n",
       "      <td>70.491296</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>812</td>\n",
       "      <td>2004-01-08</td>\n",
       "      <td>17.611557</td>\n",
       "      <td>17.674953</td>\n",
       "      <td>17.459406</td>\n",
       "      <td>17.643255</td>\n",
       "      <td>1334900</td>\n",
       "      <td>63.748235</td>\n",
       "      <td>68.320334</td>\n",
       "      <td>73.474718</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>813</td>\n",
       "      <td>2004-01-09</td>\n",
       "      <td>17.592536</td>\n",
       "      <td>17.877821</td>\n",
       "      <td>17.510121</td>\n",
       "      <td>17.732008</td>\n",
       "      <td>2369800</td>\n",
       "      <td>51.689542</td>\n",
       "      <td>63.117682</td>\n",
       "      <td>67.590673</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>814</td>\n",
       "      <td>2004-01-12</td>\n",
       "      <td>17.789070</td>\n",
       "      <td>17.839787</td>\n",
       "      <td>17.719335</td>\n",
       "      <td>17.782730</td>\n",
       "      <td>820000</td>\n",
       "      <td>61.661799</td>\n",
       "      <td>64.749230</td>\n",
       "      <td>71.158897</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>815</td>\n",
       "      <td>2004-01-13</td>\n",
       "      <td>17.846125</td>\n",
       "      <td>17.960239</td>\n",
       "      <td>17.751031</td>\n",
       "      <td>17.789068</td>\n",
       "      <td>1003000</td>\n",
       "      <td>64.006560</td>\n",
       "      <td>62.934676</td>\n",
       "      <td>66.026340</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>816</td>\n",
       "      <td>2004-01-14</td>\n",
       "      <td>17.782722</td>\n",
       "      <td>17.782722</td>\n",
       "      <td>17.630570</td>\n",
       "      <td>17.744684</td>\n",
       "      <td>830200</td>\n",
       "      <td>62.991169</td>\n",
       "      <td>58.111141</td>\n",
       "      <td>57.376842</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>817</td>\n",
       "      <td>2004-01-15</td>\n",
       "      <td>17.782727</td>\n",
       "      <td>17.827105</td>\n",
       "      <td>17.440386</td>\n",
       "      <td>17.446726</td>\n",
       "      <td>1024100</td>\n",
       "      <td>47.058603</td>\n",
       "      <td>63.999377</td>\n",
       "      <td>58.823626</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>818</td>\n",
       "      <td>2004-01-16</td>\n",
       "      <td>17.560844</td>\n",
       "      <td>17.655938</td>\n",
       "      <td>17.421370</td>\n",
       "      <td>17.611561</td>\n",
       "      <td>355600</td>\n",
       "      <td>37.736816</td>\n",
       "      <td>31.788480</td>\n",
       "      <td>48.484904</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>819</td>\n",
       "      <td>2004-01-20</td>\n",
       "      <td>17.782731</td>\n",
       "      <td>18.074355</td>\n",
       "      <td>17.719335</td>\n",
       "      <td>18.055336</td>\n",
       "      <td>956200</td>\n",
       "      <td>63.076888</td>\n",
       "      <td>57.807452</td>\n",
       "      <td>49.999962</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>820</td>\n",
       "      <td>2004-01-21</td>\n",
       "      <td>18.055340</td>\n",
       "      <td>18.232851</td>\n",
       "      <td>18.010963</td>\n",
       "      <td>18.163115</td>\n",
       "      <td>748000</td>\n",
       "      <td>73.776387</td>\n",
       "      <td>72.281057</td>\n",
       "      <td>67.785240</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>821</td>\n",
       "      <td>2004-01-22</td>\n",
       "      <td>18.258202</td>\n",
       "      <td>18.289901</td>\n",
       "      <td>17.960238</td>\n",
       "      <td>18.074352</td>\n",
       "      <td>425100</td>\n",
       "      <td>73.591356</td>\n",
       "      <td>77.537359</td>\n",
       "      <td>76.304440</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>822</td>\n",
       "      <td>2004-01-23</td>\n",
       "      <td>18.099702</td>\n",
       "      <td>18.321590</td>\n",
       "      <td>18.099702</td>\n",
       "      <td>18.232836</td>\n",
       "      <td>1697300</td>\n",
       "      <td>59.458915</td>\n",
       "      <td>61.604253</td>\n",
       "      <td>67.092226</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>823</td>\n",
       "      <td>2004-01-26</td>\n",
       "      <td>18.131406</td>\n",
       "      <td>18.378652</td>\n",
       "      <td>18.099707</td>\n",
       "      <td>18.340614</td>\n",
       "      <td>683900</td>\n",
       "      <td>65.909133</td>\n",
       "      <td>62.331539</td>\n",
       "      <td>64.049531</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>824</td>\n",
       "      <td>2004-01-27</td>\n",
       "      <td>18.384993</td>\n",
       "      <td>18.454729</td>\n",
       "      <td>18.258199</td>\n",
       "      <td>18.302578</td>\n",
       "      <td>754500</td>\n",
       "      <td>73.094061</td>\n",
       "      <td>72.932315</td>\n",
       "      <td>69.659122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>825</td>\n",
       "      <td>2004-01-28</td>\n",
       "      <td>18.289901</td>\n",
       "      <td>18.334279</td>\n",
       "      <td>18.023636</td>\n",
       "      <td>18.042654</td>\n",
       "      <td>940500</td>\n",
       "      <td>78.282622</td>\n",
       "      <td>66.666751</td>\n",
       "      <td>66.769423</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>826</td>\n",
       "      <td>2004-01-29</td>\n",
       "      <td>18.163114</td>\n",
       "      <td>18.207491</td>\n",
       "      <td>17.941226</td>\n",
       "      <td>18.029982</td>\n",
       "      <td>3247100</td>\n",
       "      <td>65.217596</td>\n",
       "      <td>70.992508</td>\n",
       "      <td>61.299738</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>827</td>\n",
       "      <td>2004-01-30</td>\n",
       "      <td>18.004611</td>\n",
       "      <td>18.055328</td>\n",
       "      <td>17.820761</td>\n",
       "      <td>17.858799</td>\n",
       "      <td>613500</td>\n",
       "      <td>46.107128</td>\n",
       "      <td>57.370060</td>\n",
       "      <td>63.635898</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>828</td>\n",
       "      <td>2004-02-02</td>\n",
       "      <td>17.903188</td>\n",
       "      <td>18.049000</td>\n",
       "      <td>17.763716</td>\n",
       "      <td>17.960245</td>\n",
       "      <td>938900</td>\n",
       "      <td>30.161415</td>\n",
       "      <td>42.619959</td>\n",
       "      <td>53.743005</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>829</td>\n",
       "      <td>2004-02-03</td>\n",
       "      <td>17.846124</td>\n",
       "      <td>17.966577</td>\n",
       "      <td>17.795406</td>\n",
       "      <td>17.890501</td>\n",
       "      <td>434500</td>\n",
       "      <td>34.143106</td>\n",
       "      <td>28.754202</td>\n",
       "      <td>40.894955</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>830</td>\n",
       "      <td>2004-02-04</td>\n",
       "      <td>17.846121</td>\n",
       "      <td>17.877820</td>\n",
       "      <td>17.662272</td>\n",
       "      <td>17.763706</td>\n",
       "      <td>536000</td>\n",
       "      <td>32.000012</td>\n",
       "      <td>34.615795</td>\n",
       "      <td>29.032418</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>831</td>\n",
       "      <td>2004-02-05</td>\n",
       "      <td>17.744697</td>\n",
       "      <td>17.782734</td>\n",
       "      <td>17.446733</td>\n",
       "      <td>17.605225</td>\n",
       "      <td>1040200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.842618</td>\n",
       "      <td>30.347594</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>832</td>\n",
       "      <td>2004-02-06</td>\n",
       "      <td>17.630579</td>\n",
       "      <td>17.725674</td>\n",
       "      <td>17.529145</td>\n",
       "      <td>17.725674</td>\n",
       "      <td>515900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.757028</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>833</td>\n",
       "      <td>2004-02-09</td>\n",
       "      <td>17.751032</td>\n",
       "      <td>18.004618</td>\n",
       "      <td>17.751032</td>\n",
       "      <td>17.941221</td>\n",
       "      <td>739300</td>\n",
       "      <td>21.348135</td>\n",
       "      <td>17.569339</td>\n",
       "      <td>15.431512</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>834</td>\n",
       "      <td>2004-02-10</td>\n",
       "      <td>17.972915</td>\n",
       "      <td>18.220161</td>\n",
       "      <td>17.947556</td>\n",
       "      <td>18.182123</td>\n",
       "      <td>941800</td>\n",
       "      <td>50.833431</td>\n",
       "      <td>41.598479</td>\n",
       "      <td>36.195981</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>835</td>\n",
       "      <td>2004-02-11</td>\n",
       "      <td>18.131411</td>\n",
       "      <td>18.448394</td>\n",
       "      <td>18.087034</td>\n",
       "      <td>18.429375</td>\n",
       "      <td>2014100</td>\n",
       "      <td>66.141440</td>\n",
       "      <td>58.499957</td>\n",
       "      <td>49.571025</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>836</td>\n",
       "      <td>2004-02-12</td>\n",
       "      <td>18.416691</td>\n",
       "      <td>18.556164</td>\n",
       "      <td>18.334276</td>\n",
       "      <td>18.486427</td>\n",
       "      <td>782900</td>\n",
       "      <td>79.640872</td>\n",
       "      <td>75.357759</td>\n",
       "      <td>68.853994</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>837</td>\n",
       "      <td>2004-02-13</td>\n",
       "      <td>18.467415</td>\n",
       "      <td>18.518132</td>\n",
       "      <td>18.315263</td>\n",
       "      <td>18.442057</td>\n",
       "      <td>860300</td>\n",
       "      <td>79.714116</td>\n",
       "      <td>79.681597</td>\n",
       "      <td>75.587779</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>838</td>\n",
       "      <td>2004-02-17</td>\n",
       "      <td>18.575179</td>\n",
       "      <td>18.632236</td>\n",
       "      <td>18.524462</td>\n",
       "      <td>18.600538</td>\n",
       "      <td>532700</td>\n",
       "      <td>89.436209</td>\n",
       "      <td>81.704308</td>\n",
       "      <td>81.664201</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>839</td>\n",
       "      <td>2004-02-18</td>\n",
       "      <td>18.619559</td>\n",
       "      <td>18.632237</td>\n",
       "      <td>18.353294</td>\n",
       "      <td>18.404011</td>\n",
       "      <td>754000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>89.723718</td>\n",
       "      <td>82.199288</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30 rows × 1280 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0   timestamp       open       high        low      close  \\\n",
       "0          810  2004-01-06  17.814430  17.833448  17.662278  17.776392   \n",
       "1          811  2004-01-07  17.681286  17.681286  17.484757  17.592531   \n",
       "2          812  2004-01-08  17.611557  17.674953  17.459406  17.643255   \n",
       "3          813  2004-01-09  17.592536  17.877821  17.510121  17.732008   \n",
       "4          814  2004-01-12  17.789070  17.839787  17.719335  17.782730   \n",
       "5          815  2004-01-13  17.846125  17.960239  17.751031  17.789068   \n",
       "6          816  2004-01-14  17.782722  17.782722  17.630570  17.744684   \n",
       "7          817  2004-01-15  17.782727  17.827105  17.440386  17.446726   \n",
       "8          818  2004-01-16  17.560844  17.655938  17.421370  17.611561   \n",
       "9          819  2004-01-20  17.782731  18.074355  17.719335  18.055336   \n",
       "10         820  2004-01-21  18.055340  18.232851  18.010963  18.163115   \n",
       "11         821  2004-01-22  18.258202  18.289901  17.960238  18.074352   \n",
       "12         822  2004-01-23  18.099702  18.321590  18.099702  18.232836   \n",
       "13         823  2004-01-26  18.131406  18.378652  18.099707  18.340614   \n",
       "14         824  2004-01-27  18.384993  18.454729  18.258199  18.302578   \n",
       "15         825  2004-01-28  18.289901  18.334279  18.023636  18.042654   \n",
       "16         826  2004-01-29  18.163114  18.207491  17.941226  18.029982   \n",
       "17         827  2004-01-30  18.004611  18.055328  17.820761  17.858799   \n",
       "18         828  2004-02-02  17.903188  18.049000  17.763716  17.960245   \n",
       "19         829  2004-02-03  17.846124  17.966577  17.795406  17.890501   \n",
       "20         830  2004-02-04  17.846121  17.877820  17.662272  17.763706   \n",
       "21         831  2004-02-05  17.744697  17.782734  17.446733  17.605225   \n",
       "22         832  2004-02-06  17.630579  17.725674  17.529145  17.725674   \n",
       "23         833  2004-02-09  17.751032  18.004618  17.751032  17.941221   \n",
       "24         834  2004-02-10  17.972915  18.220161  17.947556  18.182123   \n",
       "25         835  2004-02-11  18.131411  18.448394  18.087034  18.429375   \n",
       "26         836  2004-02-12  18.416691  18.556164  18.334276  18.486427   \n",
       "27         837  2004-02-13  18.467415  18.518132  18.315263  18.442057   \n",
       "28         838  2004-02-17  18.575179  18.632236  18.524462  18.600538   \n",
       "29         839  2004-02-18  18.619559  18.632237  18.353294  18.404011   \n",
       "\n",
       "     volume       rsi_6      rsi_7      rsi_8  ...  eom_18_EURUSD=X  \\\n",
       "0    691100   91.334101  81.891705  82.829501  ...              0.0   \n",
       "1   1676800   72.556789  77.477096  70.491296  ...              0.0   \n",
       "2   1334900   63.748235  68.320334  73.474718  ...              0.0   \n",
       "3   2369800   51.689542  63.117682  67.590673  ...              0.0   \n",
       "4    820000   61.661799  64.749230  71.158897  ...              0.0   \n",
       "5   1003000   64.006560  62.934676  66.026340  ...              0.0   \n",
       "6    830200   62.991169  58.111141  57.376842  ...              0.0   \n",
       "7   1024100   47.058603  63.999377  58.823626  ...              0.0   \n",
       "8    355600   37.736816  31.788480  48.484904  ...              0.0   \n",
       "9    956200   63.076888  57.807452  49.999962  ...              0.0   \n",
       "10   748000   73.776387  72.281057  67.785240  ...              0.0   \n",
       "11   425100   73.591356  77.537359  76.304440  ...              0.0   \n",
       "12  1697300   59.458915  61.604253  67.092226  ...              0.0   \n",
       "13   683900   65.909133  62.331539  64.049531  ...              0.0   \n",
       "14   754500   73.094061  72.932315  69.659122  ...              0.0   \n",
       "15   940500   78.282622  66.666751  66.769423  ...              0.0   \n",
       "16  3247100   65.217596  70.992508  61.299738  ...              0.0   \n",
       "17   613500   46.107128  57.370060  63.635898  ...              0.0   \n",
       "18   938900   30.161415  42.619959  53.743005  ...              0.0   \n",
       "19   434500   34.143106  28.754202  40.894955  ...              0.0   \n",
       "20   536000   32.000012  34.615795  29.032418  ...              0.0   \n",
       "21  1040200    0.000000  27.842618  30.347594  ...              0.0   \n",
       "22   515900    0.000000   0.000000  24.757028  ...              0.0   \n",
       "23   739300   21.348135  17.569339  15.431512  ...              0.0   \n",
       "24   941800   50.833431  41.598479  36.195981  ...              0.0   \n",
       "25  2014100   66.141440  58.499957  49.571025  ...              0.0   \n",
       "26   782900   79.640872  75.357759  68.853994  ...              0.0   \n",
       "27   860300   79.714116  79.681597  75.587779  ...              0.0   \n",
       "28   532700   89.436209  81.704308  81.664201  ...              0.0   \n",
       "29   754000  100.000000  89.723718  82.199288  ...              0.0   \n",
       "\n",
       "    eom_19_EURUSD=X  eom_20_EURUSD=X  eom_21_EURUSD=X  eom_22_EURUSD=X  \\\n",
       "0               0.0              0.0              0.0              0.0   \n",
       "1               0.0              0.0              0.0              0.0   \n",
       "2               0.0              0.0              0.0              0.0   \n",
       "3               0.0              0.0              0.0              0.0   \n",
       "4               0.0              0.0              0.0              0.0   \n",
       "5               0.0              0.0              0.0              0.0   \n",
       "6               0.0              0.0              0.0              0.0   \n",
       "7               0.0              0.0              0.0              0.0   \n",
       "8               0.0              0.0              0.0              0.0   \n",
       "9               0.0              0.0              0.0              0.0   \n",
       "10              0.0              0.0              0.0              0.0   \n",
       "11              0.0              0.0              0.0              0.0   \n",
       "12              0.0              0.0              0.0              0.0   \n",
       "13              0.0              0.0              0.0              0.0   \n",
       "14              0.0              0.0              0.0              0.0   \n",
       "15              0.0              0.0              0.0              0.0   \n",
       "16              0.0              0.0              0.0              0.0   \n",
       "17              0.0              0.0              0.0              0.0   \n",
       "18              0.0              0.0              0.0              0.0   \n",
       "19              0.0              0.0              0.0              0.0   \n",
       "20              0.0              0.0              0.0              0.0   \n",
       "21              0.0              0.0              0.0              0.0   \n",
       "22              0.0              0.0              0.0              0.0   \n",
       "23              0.0              0.0              0.0              0.0   \n",
       "24              0.0              0.0              0.0              0.0   \n",
       "25              0.0              0.0              0.0              0.0   \n",
       "26              0.0              0.0              0.0              0.0   \n",
       "27              0.0              0.0              0.0              0.0   \n",
       "28              0.0              0.0              0.0              0.0   \n",
       "29              0.0              0.0              0.0              0.0   \n",
       "\n",
       "    eom_23_EURUSD=X  eom_24_EURUSD=X  eom_25_EURUSD=X  eom_26_EURUSD=X  labels  \n",
       "0               0.0              0.0              0.0              0.0       2  \n",
       "1               0.0              0.0              0.0              0.0       2  \n",
       "2               0.0              0.0              0.0              0.0       2  \n",
       "3               0.0              0.0              0.0              0.0       2  \n",
       "4               0.0              0.0              0.0              0.0       2  \n",
       "5               0.0              0.0              0.0              0.0       2  \n",
       "6               0.0              0.0              0.0              0.0       2  \n",
       "7               0.0              0.0              0.0              0.0       1  \n",
       "8               0.0              0.0              0.0              0.0       2  \n",
       "9               0.0              0.0              0.0              0.0       2  \n",
       "10              0.0              0.0              0.0              0.0       2  \n",
       "11              0.0              0.0              0.0              0.0       2  \n",
       "12              0.0              0.0              0.0              0.0       2  \n",
       "13              0.0              0.0              0.0              0.0       0  \n",
       "14              0.0              0.0              0.0              0.0       2  \n",
       "15              0.0              0.0              0.0              0.0       2  \n",
       "16              0.0              0.0              0.0              0.0       2  \n",
       "17              0.0              0.0              0.0              0.0       2  \n",
       "18              0.0              0.0              0.0              0.0       2  \n",
       "19              0.0              0.0              0.0              0.0       2  \n",
       "20              0.0              0.0              0.0              0.0       2  \n",
       "21              0.0              0.0              0.0              0.0       1  \n",
       "22              0.0              0.0              0.0              0.0       2  \n",
       "23              0.0              0.0              0.0              0.0       2  \n",
       "24              0.0              0.0              0.0              0.0       2  \n",
       "25              0.0              0.0              0.0              0.0       2  \n",
       "26              0.0              0.0              0.0              0.0       2  \n",
       "27              0.0              0.0              0.0              0.0       2  \n",
       "28              0.0              0.0              0.0              0.0       0  \n",
       "29              0.0              0.0              0.0              0.0       2  \n",
       "\n",
       "[30 rows x 1280 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "# from IPython.core.interactiveshell import InteractiveShell # TODO do we need this?\n",
    "\n",
    "np.random.seed(42)\n",
    "symbol = 'XLE'\n",
    "\n",
    "# after generating dataset in cell above, import dataset\n",
    "df = pd.read_csv(\"data/df_\"+symbol+\"_aux.csv\")\n",
    "df = df[df['timestamp'] < '2016-01-01']  # For backtesting the last 5 years will be excluded from training.\n",
    "df['labels'] = df['labels'].astype(np.int8)\n",
    "# if 'dividend_amount' in df.columns:\n",
    "#   df.drop(columns=['dividend_amount', 'split_coefficient'], inplace=True)\n",
    "display(df.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['eom_26', 'unnamed: 0_CL=F', 'open_CL=F', 'high_CL=F', 'low_CL=F',\n",
       "       'close_CL=F', 'volume_CL=F', 'rsi_6_CL=F', 'rsi_7_CL=F', 'rsi_8_CL=F',\n",
       "       ...\n",
       "       'eom_18_EURUSD=X', 'eom_19_EURUSD=X', 'eom_20_EURUSD=X',\n",
       "       'eom_21_EURUSD=X', 'eom_22_EURUSD=X', 'eom_23_EURUSD=X',\n",
       "       'eom_24_EURUSD=X', 'eom_25_EURUSD=X', 'eom_26_EURUSD=X', 'labels'],\n",
       "      dtype='object', length=854)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns[426:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of features 1277\n",
      "Shape of x, y train/cv/test (1899, 1277) (1899,) (475, 1277) (475,) (594, 1277) (594,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "#[:, 'open':'eom_26']\n",
    "list_features = list(df.loc[:, 'open':'eom_26_EURUSD=X'].columns)\n",
    "print('Total number of features', len(list_features))\n",
    "x_train, x_test, y_train, y_test = train_test_split(df.loc[:, 'open':'eom_26_EURUSD=X'].values, df['labels'].values, train_size=0.8, \n",
    "                                                    test_size=0.2, random_state=2, shuffle=True, stratify=df['labels'].values)\n",
    "x_train, x_cv, y_train, y_cv = train_test_split(x_train, y_train, train_size=0.8, test_size=0.2, \n",
    "                                                random_state=2, shuffle=True, stratify=y_train)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1)) # or StandardScaler?\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_cv = scaler.transform(x_cv)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "x_main = x_train.copy()\n",
    "print(\"Shape of x, y train/cv/test {} {} {} {} {} {}\".format(x_train.shape, y_train.shape, x_cv.shape, y_cv.shape, x_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 225  # should be a perfect square\n",
    "selection_method = 'all'\n",
    "topk = 320 if selection_method == 'all' else num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:114: UserWarning: Features [ 856  899  900  901  902  903  904  905  906  907  908  909  910  911\n",
      "  912  913  914  915  916  917  918  919  941  942  943  944  945  946\n",
      "  947  948  949  950  951  952  953  954  955  956  957  958  959  960\n",
      "  961 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205\n",
      " 1206 1207 1208 1209 1210 1211 1212 1213 1256 1257 1258 1259 1260 1261\n",
      " 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275\n",
      " 1276] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx,\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:116: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('rsi_6', 'rsi_7', 'rsi_8', 'rsi_9', 'rsi_10', 'rsi_11', 'rsi_12', 'rsi_13', 'rsi_14', 'rsi_15', 'rsi_16', 'wr_6', 'wr_7', 'wr_8', 'wr_9', 'wr_10', 'wr_11', 'wr_12', 'wr_13', 'wr_14', 'wr_15', 'wr_16', 'wr_17', 'wr_18', 'wr_19', 'wr_20', 'wr_21', 'wr_22', 'wr_23', 'wr_24', 'wr_25', 'wr_26', 'mfi_6', 'mfi_7', 'mfi_8', 'mfi_9', 'mfi_10', 'mfi_11', 'mfi_12', 'mfi_13', 'mfi_14', 'mfi_15', 'mfi_16', 'mfi_17', 'mfi_18', 'mfi_19', 'mfi_20', 'mfi_21', 'mfi_22', 'mfi_23', 'mfi_24', 'mfi_25', 'roc_6', 'roc_7', 'roc_8', 'roc_9', 'roc_10', 'roc_11', 'roc_12', 'roc_13', 'roc_14', 'roc_15', 'roc_16', 'roc_17', 'roc_18', 'roc_19', 'roc_20', 'roc_21', 'roc_22', 'roc_23', 'roc_24', 'roc_25', 'roc_26', 'cmf_6', 'cmf_7', 'cmf_8', 'cmf_9', 'cmf_10', 'cmf_11', 'cmf_12', 'cmf_13', 'cmf_14', 'cmf_15', 'cmf_16', 'cmf_17', 'cmf_18', 'cmf_20', 'cmo_6', 'cmo_7', 'cmo_8', 'cmo_9', 'cmo_10', 'cmo_11', 'cmo_12', 'cmo_13', 'cmo_14', 'cmo_15', 'cmo_16', 'cmo_17', 'trix_6', 'cci_6', 'cci_7', 'cci_8', 'cci_9', 'cci_10', 'cci_11', 'cci_12', 'cci_13', 'cci_14', 'cci_15', 'cci_16', 'cci_17', 'cci_18', 'cci_19', 'cci_20', 'cci_21', 'cci_22', 'cci_23', 'cci_24', 'cci_25', 'cci_26', 'dpo_6', 'dpo_7', 'dpo_8', 'dpo_9', 'dpo_10', 'dpo_11', 'dpo_12', 'fi_6', 'fi_7', 'fi_8', 'fi_9', 'fi_10', 'fi_11', 'fi_12', 'fi_13', 'fi_14', 'fi_15', 'fi_16', 'fi_17', 'fi_18', 'fi_19', 'fi_20', 'fi_21', 'fi_22', 'fi_23', 'fi_24', 'fi_25', 'fi_26', 'rsv_6', 'kdjk_6', 'rsv_7', 'kdjk_7', 'rsv_8', 'kdjk_8', 'rsv_9', 'kdjk_9', 'rsv_10', 'kdjk_10', 'rsv_11', 'kdjk_11', 'rsv_12', 'kdjk_12', 'rsv_13', 'kdjk_13', 'rsv_14', 'kdjk_14', 'rsv_15', 'kdjk_15', 'rsv_16', 'kdjk_16', 'rsv_17', 'kdjk_17', 'rsv_18', 'kdjk_18', 'rsv_19', 'kdjk_19', 'rsv_20', 'kdjk_20', 'rsv_21', 'kdjk_21', 'rsv_22', 'kdjk_22', 'rsv_23', 'kdjk_23', 'rsv_24', 'kdjk_24', 'rsv_25', 'kdjk_25', 'rsv_26', 'kdjk_26', 'eom_6', 'eom_7', 'eom_8', 'eom_9', 'eom_10', 'eom_11', 'eom_12', 'eom_13', 'eom_14', 'eom_15', 'eom_16', 'eom_17', 'eom_18', 'eom_19', 'eom_20', 'eom_21', 'eom_22', 'eom_23', 'eom_24', 'eom_25', 'eom_26', 'wr_6_CL=F', 'wr_7_CL=F', 'wr_8_CL=F', 'wr_9_CL=F', 'wr_10_CL=F', 'wr_11_CL=F', 'wr_12_CL=F', 'wr_13_CL=F', 'wr_14_CL=F', 'wr_15_CL=F', 'wr_16_CL=F', 'wr_17_CL=F', 'wr_18_CL=F', 'wr_19_CL=F', 'wr_20_CL=F', 'wr_21_CL=F', 'wr_22_CL=F', 'wr_23_CL=F', 'wr_24_CL=F', 'wr_25_CL=F', 'wr_26_CL=F', 'mfi_6_CL=F', 'mfi_7_CL=F', 'mfi_8_CL=F', 'roc_6_CL=F', 'roc_7_CL=F', 'roc_8_CL=F', 'roc_9_CL=F', 'roc_10_CL=F', 'roc_11_CL=F', 'roc_12_CL=F', 'roc_13_CL=F', 'roc_14_CL=F', 'roc_15_CL=F', 'cmf_6_CL=F', 'cmf_7_CL=F', 'cci_6_CL=F', 'cci_7_CL=F', 'cci_8_CL=F', 'cci_9_CL=F', 'cci_10_CL=F', 'cci_11_CL=F', 'cci_12_CL=F', 'cci_13_CL=F', 'cci_14_CL=F', 'cci_15_CL=F', 'cci_16_CL=F', 'cci_17_CL=F', 'cci_18_CL=F', 'cci_19_CL=F', 'cci_20_CL=F', 'cci_21_CL=F', 'cci_22_CL=F', 'cci_23_CL=F', 'cci_24_CL=F', 'cci_25_CL=F', 'cci_26_CL=F', 'dpo_6_CL=F', 'dpo_7_CL=F', 'fi_6_CL=F', 'fi_7_CL=F', 'fi_8_CL=F', 'fi_9_CL=F', 'fi_10_CL=F', 'fi_11_CL=F', 'fi_12_CL=F', 'fi_13_CL=F', 'fi_14_CL=F', 'fi_15_CL=F', 'fi_16_CL=F', 'fi_17_CL=F', 'fi_18_CL=F', 'fi_19_CL=F', 'fi_20_CL=F', 'fi_21_CL=F', 'fi_22_CL=F', 'fi_23_CL=F', 'fi_24_CL=F', 'fi_25_CL=F', 'fi_26_CL=F', 'rsv_6_CL=F', 'kdjk_6_CL=F', 'rsv_7_CL=F', 'kdjk_7_CL=F', 'rsv_8_CL=F', 'kdjk_8_CL=F', 'rsv_9_CL=F', 'kdjk_9_CL=F', 'rsv_10_CL=F', 'kdjk_10_CL=F', 'rsv_11_CL=F', 'kdjk_11_CL=F', 'rsv_12_CL=F', 'kdjk_12_CL=F', 'rsv_13_CL=F', 'rsv_14_CL=F', 'rsv_15_CL=F', 'rsv_16_CL=F', 'rsv_17_CL=F', 'rsv_18_CL=F', 'rsv_19_CL=F', 'rsv_20_CL=F', 'rsv_21_CL=F', 'rsv_22_CL=F', 'rsv_23_CL=F', 'rsv_24_CL=F', 'rsv_25_CL=F', 'rsv_26_CL=F')\n",
      "[  5   6   7   8   9  10  11  12  13  14  15  26  27  28  29  30  31  32\n",
      "  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50\n",
      "  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  68  69\n",
      "  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87\n",
      "  88  89  90  91  92  93  94  95  96  97  98  99 100 101 103 110 111 112\n",
      " 113 114 115 116 117 118 119 120 121 215 236 237 238 239 240 241 242 243\n",
      " 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261\n",
      " 262 263 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356\n",
      " 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374\n",
      " 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392\n",
      " 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410\n",
      " 411 412 413 414 415 416 417 418 419 420 421 422 423 424 452 453 454 455\n",
      " 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473\n",
      " 474 475 494 495 496 497 498 499 500 501 502 503 515 516 662 663 664 665\n",
      " 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683\n",
      " 684 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783\n",
      " 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801\n",
      " 802 804 806 808 810 812 814 816 818 820 822 824 826 828]\n",
      "****************************************\n",
      "320 ('rsi_6', 'rsi_7', 'rsi_8', 'rsi_10', 'rsi_14', 'rsi_19', 'rsi_26', 'wr_6', 'wr_7', 'wr_8', 'wr_9', 'wr_10', 'wr_11', 'wr_12', 'wr_13', 'wr_14', 'wr_15', 'wr_16', 'wr_17', 'wr_18', 'wr_19', 'wr_20', 'wr_21', 'wr_22', 'wr_23', 'wr_24', 'wr_25', 'wr_26', 'mfi_6', 'mfi_7', 'mfi_8', 'mfi_9', 'mfi_10', 'mfi_11', 'mfi_12', 'mfi_15', 'mfi_17', 'mfi_23', 'roc_6', 'roc_7', 'roc_8', 'roc_9', 'roc_10', 'roc_12', 'roc_13', 'roc_14', 'roc_15', 'roc_16', 'roc_17', 'roc_18', 'roc_19', 'roc_20', 'roc_22', 'roc_23', 'roc_25', 'roc_26', 'cmf_6', 'cmf_7', 'cmf_8', 'cmf_10', 'cmf_12', 'cmf_13', 'cmf_14', 'cmf_15', 'cmf_16', 'cmf_17', 'cmf_18', 'cmo_6', 'cmo_7', 'cmo_9', 'cmo_15', 'cmo_16', 'cmo_18', 'cmo_20', 'trix_6', 'cci_6', 'cci_7', 'cci_8', 'cci_9', 'cci_10', 'cci_11', 'cci_12', 'cci_13', 'cci_14', 'cci_15', 'cci_16', 'cci_17', 'cci_18', 'cci_19', 'cci_20', 'cci_21', 'cci_22', 'cci_23', 'cci_24', 'cci_25', 'cci_26', 'dpo_6', 'dpo_7', 'dpo_8', 'dpo_9', 'dmi_7', 'dmi_15', 'fi_6', 'fi_7', 'fi_8', 'fi_9', 'fi_10', 'fi_11', 'fi_12', 'fi_13', 'fi_14', 'fi_15', 'fi_16', 'fi_17', 'fi_18', 'fi_19', 'fi_20', 'fi_21', 'fi_22', 'fi_23', 'fi_24', 'fi_25', 'fi_26', 'rsv_6', 'kdjk_6', 'rsv_7', 'kdjk_7', 'rsv_8', 'kdjk_8', 'rsv_9', 'kdjk_9', 'rsv_10', 'kdjk_10', 'rsv_11', 'kdjk_11', 'rsv_12', 'kdjk_12', 'rsv_13', 'kdjk_13', 'rsv_14', 'kdjk_14', 'rsv_15', 'kdjk_15', 'rsv_16', 'kdjk_16', 'rsv_17', 'kdjk_17', 'rsv_18', 'kdjk_18', 'rsv_19', 'kdjk_19', 'rsv_20', 'kdjk_20', 'rsv_21', 'kdjk_21', 'rsv_22', 'kdjk_22', 'rsv_23', 'kdjk_23', 'rsv_24', 'kdjk_24', 'rsv_25', 'kdjk_25', 'rsv_26', 'eom_6', 'eom_7', 'eom_8', 'eom_9', 'eom_10', 'eom_11', 'eom_12', 'eom_13', 'eom_14', 'eom_15', 'eom_16', 'eom_17', 'eom_18', 'eom_19', 'eom_20', 'eom_21', 'eom_22', 'eom_23', 'eom_24', 'eom_25', 'eom_26', 'wr_6_CL=F', 'wr_7_CL=F', 'wr_8_CL=F', 'wr_9_CL=F', 'wr_10_CL=F', 'wr_11_CL=F', 'wr_12_CL=F', 'wr_13_CL=F', 'wr_14_CL=F', 'wr_15_CL=F', 'wr_16_CL=F', 'wr_17_CL=F', 'wr_18_CL=F', 'wr_19_CL=F', 'wr_20_CL=F', 'wr_21_CL=F', 'wr_22_CL=F', 'wr_23_CL=F', 'wr_24_CL=F', 'wr_25_CL=F', 'wr_26_CL=F', 'mfi_7_CL=F', 'roc_6_CL=F', 'roc_7_CL=F', 'roc_9_CL=F', 'roc_10_CL=F', 'roc_12_CL=F', 'roc_15_CL=F', 'cmf_10_CL=F', 'cmf_13_CL=F', 'cmo_12_CL=F', 'cmo_14_CL=F', 'trix_14_CL=F', 'cci_6_CL=F', 'cci_7_CL=F', 'cci_8_CL=F', 'cci_9_CL=F', 'cci_10_CL=F', 'cci_11_CL=F', 'cci_12_CL=F', 'cci_13_CL=F', 'cci_14_CL=F', 'cci_15_CL=F', 'cci_16_CL=F', 'cci_17_CL=F', 'cci_18_CL=F', 'cci_20_CL=F', 'cci_21_CL=F', 'cci_22_CL=F', 'cci_23_CL=F', 'cci_25_CL=F', 'cci_26_CL=F', 'dpo_23_CL=F', 'fi_6_CL=F', 'fi_7_CL=F', 'fi_8_CL=F', 'fi_9_CL=F', 'fi_10_CL=F', 'fi_11_CL=F', 'fi_12_CL=F', 'fi_13_CL=F', 'fi_14_CL=F', 'fi_15_CL=F', 'fi_16_CL=F', 'fi_17_CL=F', 'fi_18_CL=F', 'fi_19_CL=F', 'fi_20_CL=F', 'fi_21_CL=F', 'fi_22_CL=F', 'fi_23_CL=F', 'fi_24_CL=F', 'fi_25_CL=F', 'fi_26_CL=F', 'rsv_6_CL=F', 'rsv_7_CL=F', 'kdjk_7_CL=F', 'rsv_8_CL=F', 'rsv_9_CL=F', 'kdjk_9_CL=F', 'rsv_10_CL=F', 'rsv_11_CL=F', 'rsv_12_CL=F', 'rsv_13_CL=F', 'kdjk_13_CL=F', 'rsv_14_CL=F', 'kdjk_14_CL=F', 'rsv_15_CL=F', 'rsv_16_CL=F', 'kdjk_16_CL=F', 'rsv_17_CL=F', 'kdjk_17_CL=F', 'rsv_18_CL=F', 'kdjk_18_CL=F', 'rsv_19_CL=F', 'rsv_20_CL=F', 'kdjk_20_CL=F', 'rsv_21_CL=F', 'kdjk_21_CL=F', 'rsv_22_CL=F', 'rsv_23_CL=F', 'kdjk_23_CL=F', 'rsv_24_CL=F', 'rsv_25_CL=F', 'rsv_26_CL=F', 'eom_6_CL=F', 'eom_7_CL=F', 'eom_8_CL=F', 'eom_9_CL=F', 'eom_10_CL=F', 'eom_11_CL=F', 'eom_12_CL=F', 'eom_13_CL=F', 'eom_14_CL=F', 'eom_15_CL=F', 'eom_16_CL=F', 'eom_17_CL=F', 'eom_18_CL=F', 'eom_19_CL=F', 'eom_20_CL=F', 'eom_21_CL=F', 'eom_22_CL=F', 'eom_23_CL=F', 'eom_24_CL=F', 'eom_25_CL=F', 'eom_26_CL=F', 'wr_13_EURUSD=X', 'wr_15_EURUSD=X', 'wr_17_EURUSD=X', 'wr_19_EURUSD=X', 'open_sma_14_EURUSD=X', 'fi_11_EURUSD=X', 'rsv_13_EURUSD=X', 'rsv_15_EURUSD=X', 'rsv_19_EURUSD=X')\n",
      "[   5    6    7    9   13   18   25   26   27   28   29   30   31   32\n",
      "   33   34   35   36   37   38   39   40   41   42   43   44   45   46\n",
      "   47   48   49   50   51   52   53   56   58   64   68   69   70   71\n",
      "   72   74   75   76   77   78   79   80   81   82   84   85   87   88\n",
      "   89   90   91   93   95   96   97   98   99  100  101  110  111  113\n",
      "  119  120  122  124  215  236  237  238  239  240  241  242  243  244\n",
      "  245  246  247  248  249  250  251  252  253  254  255  256  257  258\n",
      "  259  260  300  308  341  342  343  344  345  346  347  348  349  350\n",
      "  351  352  353  354  355  356  357  358  359  360  361  362  363  364\n",
      "  365  366  367  368  369  370  371  372  373  374  375  376  377  378\n",
      "  379  380  381  382  383  384  385  386  387  388  389  390  391  392\n",
      "  393  394  395  396  397  398  399  400  401  402  404  405  406  407\n",
      "  408  409  410  411  412  413  414  415  416  417  418  419  420  421\n",
      "  422  423  424  452  453  454  455  456  457  458  459  460  461  462\n",
      "  463  464  465  466  467  468  469  470  471  472  474  494  495  497\n",
      "  498  500  503  519  522  542  544  649  662  663  664  665  666  667\n",
      "  668  669  670  671  672  673  674  676  677  678  679  681  682  700\n",
      "  767  768  769  770  771  772  773  774  775  776  777  778  779  780\n",
      "  781  782  783  784  785  786  787  788  790  791  792  794  795  796\n",
      "  798  800  802  803  804  805  806  808  809  810  811  812  813  814\n",
      "  816  817  818  819  820  822  823  824  826  828  830  831  832  833\n",
      "  834  835  836  837  838  839  840  841  842  843  844  845  846  847\n",
      "  848  849  850  885  887  889  891  991 1198 1228 1232 1240]\n",
      "CPU times: user 23.9 s, sys: 377 ms, total: 24.3 s\n",
      "Wall time: 26.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from operator import itemgetter\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "\n",
    "if selection_method == 'anova' or selection_method == 'all':\n",
    "    select_k_best = SelectKBest(f_classif, k=topk)\n",
    "    if selection_method != 'all':\n",
    "        x_train = select_k_best.fit_transform(x_main, y_train)\n",
    "        x_cv = select_k_best.transform(x_cv)\n",
    "        x_test = select_k_best.transform(x_test)\n",
    "    else:\n",
    "        select_k_best.fit(x_main, y_train)\n",
    "    \n",
    "    selected_features_anova = itemgetter(*select_k_best.get_support(indices=True))(list_features)\n",
    "    print(selected_features_anova)\n",
    "    print(select_k_best.get_support(indices=True))\n",
    "    print(\"****************************************\")\n",
    "    \n",
    "if selection_method == 'mutual_info' or selection_method == 'all':\n",
    "    select_k_best = SelectKBest(mutual_info_classif, k=topk)\n",
    "    if selection_method != 'all':\n",
    "        x_train = select_k_best.fit_transform(x_main, y_train)\n",
    "        x_cv = select_k_best.transform(x_cv)\n",
    "        x_test = select_k_best.transform(x_test)\n",
    "    else:\n",
    "        select_k_best.fit(x_main, y_train)\n",
    "\n",
    "    selected_features_mic = itemgetter(*select_k_best.get_support(indices=True))(list_features)\n",
    "    print(len(selected_features_mic), selected_features_mic)\n",
    "    print(select_k_best.get_support(indices=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common selected featues 270 ['kdjk_16', 'mfi_15', 'rsv_26', 'fi_22_CL=F', 'roc_15', 'wr_20', 'eom_20', 'cci_6', 'cci_7', 'roc_22', 'cci_26', 'rsv_10', 'dpo_6', 'cmo_15', 'cmf_18', 'wr_7', 'rsv_22_CL=F', 'wr_15', 'cmo_16', 'wr_7_CL=F', 'eom_16', 'rsi_6', 'wr_23_CL=F', 'kdjk_24', 'fi_9', 'wr_23', 'wr_13_CL=F', 'fi_15', 'fi_21', 'cci_14', 'eom_15', 'rsv_17_CL=F', 'wr_18_CL=F', 'rsv_21_CL=F', 'cci_12', 'cci_21_CL=F', 'fi_7_CL=F', 'eom_6', 'rsv_11_CL=F', 'wr_26_CL=F', 'wr_18', 'wr_8', 'cci_10', 'roc_6', 'rsv_19_CL=F', 'fi_24', 'wr_12', 'roc_12', 'rsv_25', 'wr_17_CL=F', 'cmf_16', 'fi_18', 'roc_9', 'eom_9', 'cmo_7', 'eom_23', 'rsi_7', 'roc_7_CL=F', 'wr_24', 'wr_14', 'cci_17_CL=F', 'fi_19_CL=F', 'kdjk_23', 'rsv_6', 'rsv_24_CL=F', 'kdjk_11', 'wr_17', 'mfi_6', 'kdjk_20', 'kdjk_7', 'cci_22', 'cci_25_CL=F', 'mfi_7', 'fi_19', 'wr_20_CL=F', 'wr_10_CL=F', 'eom_18', 'kdjk_25', 'roc_10_CL=F', 'wr_11', 'rsv_15_CL=F', 'cci_14_CL=F', 'roc_14', 'rsv_18_CL=F', 'kdjk_22', 'fi_20_CL=F', 'mfi_12', 'rsi_10', 'fi_11', 'cci_12_CL=F', 'cmf_14', 'kdjk_9', 'mfi_7_CL=F', 'cmf_8', 'rsv_12_CL=F', 'cci_7_CL=F', 'roc_26', 'wr_16', 'fi_13', 'eom_26', 'cci_9_CL=F', 'wr_13', 'cmf_12', 'fi_14_CL=F', 'fi_17_CL=F', 'eom_17', 'eom_13', 'cmo_6', 'eom_14', 'rsv_7_CL=F', 'fi_18_CL=F', 'cci_13_CL=F', 'cci_15_CL=F', 'rsv_8_CL=F', 'fi_14', 'kdjk_9_CL=F', 'rsv_22', 'fi_12', 'fi_6_CL=F', 'wr_26', 'dpo_8', 'cci_11_CL=F', 'fi_21_CL=F', 'fi_25_CL=F', 'rsv_17', 'roc_20', 'wr_10', 'rsv_10_CL=F', 'eom_25', 'cci_8_CL=F', 'mfi_23', 'cci_10_CL=F', 'fi_13_CL=F', 'roc_8', 'kdjk_14', 'eom_10', 'kdjk_19', 'roc_12_CL=F', 'wr_22', 'wr_14_CL=F', 'rsv_14', 'roc_17', 'fi_23_CL=F', 'kdjk_8', 'roc_9_CL=F', 'kdjk_7_CL=F', 'roc_16', 'cmf_6', 'cci_21', 'dpo_7', 'rsv_16_CL=F', 'cci_9', 'rsv_11', 'rsv_13', 'rsv_16', 'rsv_19', 'cmf_13', 'roc_15_CL=F', 'cci_11', 'wr_22_CL=F', 'cci_22_CL=F', 'fi_23', 'wr_11_CL=F', 'roc_23', 'eom_11', 'wr_24_CL=F', 'cci_23', 'dpo_9', 'rsv_8', 'kdjk_6', 'rsv_9', 'cci_18_CL=F', 'wr_16_CL=F', 'kdjk_17', 'wr_6', 'rsv_6_CL=F', 'fi_7', 'wr_19_CL=F', 'fi_6', 'wr_8_CL=F', 'rsv_7', 'mfi_11', 'mfi_8', 'cci_15', 'rsv_20', 'eom_19', 'wr_12_CL=F', 'rsv_26_CL=F', 'cci_16', 'mfi_9', 'cci_24', 'kdjk_21', 'rsv_23', 'fi_11_CL=F', 'rsv_23_CL=F', 'cmf_15', 'cmf_17', 'eom_22', 'roc_6_CL=F', 'wr_25_CL=F', 'roc_25', 'roc_13', 'fi_26_CL=F', 'rsv_9_CL=F', 'wr_9', 'cci_23_CL=F', 'cci_6_CL=F', 'wr_25', 'cmf_10', 'rsi_8', 'fi_20', 'cci_20_CL=F', 'eom_24', 'cci_8', 'wr_6_CL=F', 'cmo_9', 'roc_7', 'kdjk_15', 'kdjk_10', 'kdjk_18', 'wr_19', 'cci_17', 'wr_9_CL=F', 'cci_20', 'cci_19', 'fi_26', 'fi_8_CL=F', 'wr_15_CL=F', 'fi_24_CL=F', 'cci_25', 'fi_9_CL=F', 'roc_10', 'roc_19', 'rsv_15', 'eom_8', 'rsi_14', 'fi_17', 'mfi_10', 'rsv_24', 'eom_21', 'fi_12_CL=F', 'fi_16_CL=F', 'rsv_25_CL=F', 'cci_16_CL=F', 'fi_10', 'mfi_17', 'rsv_20_CL=F', 'roc_18', 'rsv_21', 'fi_25', 'trix_6', 'rsv_18', 'cci_18', 'cci_13', 'cci_26_CL=F', 'fi_22', 'rsv_13_CL=F', 'rsv_14_CL=F', 'wr_21_CL=F', 'fi_15_CL=F', 'kdjk_13', 'wr_21', 'rsv_12', 'fi_16', 'eom_7', 'cmf_7', 'eom_12', 'fi_8', 'kdjk_12', 'fi_10_CL=F']\n",
      "[5, 6, 7, 9, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 52, 53, 56, 64, 68, 69, 70, 71, 74, 75, 76, 77, 78, 79, 82, 84, 85, 87, 88, 89, 91, 93, 95, 96, 97, 98, 99, 100, 101, 110, 111, 113, 119, 120, 236, 237, 238, 239, 240, 241, 242, 244, 245, 246, 247, 249, 250, 251, 252, 253, 254, 256, 257, 258, 259, 260, 341, 342, 344, 346, 347, 348, 349, 350, 353, 354, 355, 356, 358, 359, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 376, 378, 379, 381, 382, 383, 384, 385, 387, 388, 389, 390, 391, 393, 394, 395, 396, 397, 399, 400, 401, 402, 404, 407, 408, 409, 411, 412, 413, 414, 415, 416, 417, 418, 420, 421, 422, 423, 424, 452, 453, 454, 455, 456, 457, 458, 459, 460, 462, 463, 464, 465, 466, 468, 469, 470, 471, 472, 474, 494, 495, 497, 498, 500, 503, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 673, 674, 676, 677, 678, 679, 681, 767, 768, 772, 774, 775, 778, 779, 780, 781, 782, 783, 784, 786, 787, 788, 790, 791, 792, 794, 795, 796, 798, 800, 806, 808, 810, 812, 814, 818, 820, 822, 824, 828]\n"
     ]
    }
   ],
   "source": [
    "if selection_method == 'all':\n",
    "    common = list(set(selected_features_anova).intersection(selected_features_mic))\n",
    "    print(\"common selected featues\", len(common), common)\n",
    "    if len(common) < num_features:\n",
    "        raise Exception('number of common features found {} < {} required features. Increase \"topk variable\"'.format(len(common), num_features))\n",
    "    feat_idx = []\n",
    "    for c in common:\n",
    "        feat_idx.append(list_features.index(c))\n",
    "    feat_idx = sorted(feat_idx[0:225])\n",
    "    print(feat_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x, y train/cv/test (1899, 225) (1899,) (475, 225) (475,) (594, 225) (594,)\n"
     ]
    }
   ],
   "source": [
    "if selection_method == 'all':\n",
    "    x_train = x_train[:, feat_idx]\n",
    "    x_cv = x_cv[:, feat_idx]\n",
    "    x_test = x_test[:, feat_idx]\n",
    "\n",
    "print(\"Shape of x, y train/cv/test {} {} {} {} {} {}\".format(x_train.shape, \n",
    "                                                             y_train.shape, x_cv.shape, y_cv.shape, x_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage of class 0 = 5.792522380200105, class 1 = 5.581885202738284\n"
     ]
    }
   ],
   "source": [
    "_labels, _counts = np.unique(y_train, return_counts=True)\n",
    "print(\"percentage of class 0 = {}, class 1 = {}\".format(_counts[0]/len(y_train) * 100, _counts[1]/len(y_train) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "from utils import reshape_array_as_image\n",
    "from metrics import f1_weighted, f1_metric\n",
    "\n",
    "def get_sample_weights(y): # TODO add source\n",
    "    \"\"\"\n",
    "    calculate the sample weights based on class weights. Used for models with\n",
    "    imbalanced data and one hot encoding prediction.\n",
    "\n",
    "    params:\n",
    "        y: class labels as integers\n",
    "    \"\"\"\n",
    "\n",
    "    y = y.astype(int)  # compute_class_weight needs int labels\n",
    "    class_weights = compute_class_weight('balanced', np.unique(y), y)\n",
    "    \n",
    "    print(\"real class weights are {}\".format(class_weights), np.unique(y))\n",
    "    print(\"value_counts\", np.unique(y, return_counts=True))\n",
    "    sample_weights = y.copy().astype(float)\n",
    "    for i in set(y): #np.unique\n",
    "        sample_weights[sample_weights == i] = class_weights[i]  # if i == 2 else 0.8 * class_weights[i]\n",
    "        # sample_weights = np.where(sample_weights == i, class_weights[int(i)], y_)\n",
    "\n",
    "    return sample_weights\n",
    "\n",
    "get_custom_objects().update({\"f1_metric\": f1_metric, \"f1_weighted\": f1_weighted}) # why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real class weights are [5.75454545 5.97169811 0.37611408] [0 1 2]\n",
      "value_counts (array([0, 1, 2]), array([ 110,  106, 1683]))\n",
      "Test sample_weights\n",
      "[2 2 2 2 0 1 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 0 1 1 2 2 2 2 2]\n",
      "[0.37611408 0.37611408 0.37611408 0.37611408 5.75454545 5.97169811\n",
      " 0.37611408 0.37611408 0.37611408 0.37611408 0.37611408 0.37611408\n",
      " 0.37611408 5.75454545 0.37611408 0.37611408 0.37611408 0.37611408\n",
      " 0.37611408 0.37611408 0.37611408 0.37611408 5.75454545 5.97169811\n",
      " 5.97169811 0.37611408 0.37611408 0.37611408 0.37611408 0.37611408]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass classes=[0 1 2], y=[2 2 2 ... 2 2 2] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    }
   ],
   "source": [
    "sample_weights = get_sample_weights(y_train)\n",
    "print(\"Test sample_weights\")\n",
    "rand_idx = np.random.randint(0, 1000, 30)\n",
    "print(y_train[rand_idx])\n",
    "print(sample_weights[rand_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train (1899, 3)\n"
     ]
    }
   ],
   "source": [
    "one_hot_enc = OneHotEncoder(sparse=False, categories='auto')  # , categories='auto'\n",
    "y_train = one_hot_enc.fit_transform(y_train.reshape(-1, 1))\n",
    "print(\"y_train\",y_train.shape)\n",
    "y_cv = one_hot_enc.transform(y_cv.reshape(-1, 1))\n",
    "y_test = one_hot_enc.transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(225,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final shape of x, y train/test (1899, 15, 15, 3) (1899, 3) (594, 15, 15, 3) (594, 3)\n"
     ]
    }
   ],
   "source": [
    "dim = int(np.sqrt(num_features))\n",
    "x_train = reshape_array_as_image(x_train, dim, dim)\n",
    "x_cv = reshape_array_as_image(x_cv, dim, dim)\n",
    "x_test = reshape_array_as_image(x_test, dim, dim)\n",
    "# adding a 1-dim for channels (3)\n",
    "x_train = np.stack((x_train,) * 3, axis=-1)\n",
    "x_test = np.stack((x_test,) * 3, axis=-1)\n",
    "x_cv = np.stack((x_cv,) * 3, axis=-1)\n",
    "print(\"final shape of x, y train/test {} {} {} {}\".format(x_train.shape, y_train.shape, x_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "columns = rows = 3\n",
    "for i in range(1, columns*rows +1):\n",
    "    index = np.random.randint(len(x_train))\n",
    "    img = x_train[index]\n",
    "    fig.add_subplot(rows, columns, i)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title('image_'+str(index)+'_class_'+str(np.argmax(y_train[index])), fontsize=10)\n",
    "    plt.subplots_adjust(wspace=0.2, hspace=0.2)\n",
    "    plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import print_with_timestamp, show_images\n",
    "show_images(3,3, arr=x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 15, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x158427b20>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = [i for i in range(225)]\n",
    "test = np.array(test).reshape(-1, 1)\n",
    "#test = scaler.fit_transform(test)\n",
    "dim = int(np.sqrt(num_features))\n",
    "test = np.reshape(test, (dim, dim))\n",
    "plt.imshow(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "         13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "         26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "         39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "         52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "         65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "         91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "        104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "        117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "        130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "        143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "        169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "        195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
       "        208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
       "        221, 222, 223, 224]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(test).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, LeakyReLU\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger, Callback\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.regularizers import l2, l1, l1_l2\n",
    "from tensorflow.keras.initializers import RandomUniform, RandomNormal\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "params = {'batch_size': 80, 'conv2d_layers': {'conv2d_do_1': 0.2, 'conv2d_filters_1': 32, 'conv2d_kernel_size_1': 3, 'conv2d_mp_1': 0, \n",
    "                                               'conv2d_strides_1': 1, 'kernel_regularizer_1': 0.0, 'conv2d_do_2': 0.3, \n",
    "                                               'conv2d_filters_2': 64, 'conv2d_kernel_size_2': 3, 'conv2d_mp_2': 2, 'conv2d_strides_2': 1, \n",
    "                                               'kernel_regularizer_2': 0.0, 'layers': 'two'}, \n",
    "           'dense_layers': {'dense_do_1': 0.3, 'dense_nodes_1': 128, 'kernel_regularizer_1': 0.0, 'layers': 'one'},\n",
    "           'epochs': 200, 'lr': 0.001, 'optimizer': 'adam'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import *\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras.metrics import AUC\n",
    "\n",
    "def f1_custom(y_true, y_pred):\n",
    "    y_t = np.argmax(y_true, axis=1)\n",
    "    y_p = np.argmax(y_pred, axis=1)\n",
    "    f1_score(y_t, y_p, labels=None, average='weighted', sample_weight=None, zero_division='warn')\n",
    "\n",
    "def create_model_cnn(params):\n",
    "    model = Sequential()\n",
    "\n",
    "    print(\"Training with params {}\".format(params))\n",
    "    \n",
    "    conv2d_layer1 = Conv2D(params[\"conv2d_layers\"][\"conv2d_filters_1\"],\n",
    "                           params[\"conv2d_layers\"][\"conv2d_kernel_size_1\"],\n",
    "                           strides=params[\"conv2d_layers\"][\"conv2d_strides_1\"],\n",
    "                           kernel_regularizer=regularizers.l2(params[\"conv2d_layers\"][\"kernel_regularizer_1\"]), \n",
    "                           padding='same',activation=\"relu\", use_bias=True,\n",
    "                           kernel_initializer='glorot_uniform',\n",
    "                           input_shape=(x_train[0].shape[0],\n",
    "                                        x_train[0].shape[1], x_train[0].shape[2]))\n",
    "    model.add(conv2d_layer1)\n",
    "    if params[\"conv2d_layers\"]['conv2d_mp_1'] > 1:\n",
    "        model.add(MaxPool2D(pool_size=params[\"conv2d_layers\"]['conv2d_mp_1']))\n",
    "        \n",
    "    model.add(Dropout(params['conv2d_layers']['conv2d_do_1']))\n",
    "    if params[\"conv2d_layers\"]['layers'] == 'two':\n",
    "        conv2d_layer2 = Conv2D(params[\"conv2d_layers\"][\"conv2d_filters_2\"],\n",
    "                               params[\"conv2d_layers\"][\"conv2d_kernel_size_2\"],\n",
    "                               strides=params[\"conv2d_layers\"][\"conv2d_strides_2\"],\n",
    "                               kernel_regularizer=regularizers.l2(params[\"conv2d_layers\"][\"kernel_regularizer_2\"]),\n",
    "                               padding='same',activation=\"relu\", use_bias=True,\n",
    "                               kernel_initializer='glorot_uniform')\n",
    "        model.add(conv2d_layer2)\n",
    "        \n",
    "        if params[\"conv2d_layers\"]['conv2d_mp_2'] > 1:\n",
    "            model.add(MaxPool2D(pool_size=params[\"conv2d_layers\"]['conv2d_mp_2']))\n",
    "        \n",
    "        model.add(Dropout(params['conv2d_layers']['conv2d_do_2']))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(params['dense_layers'][\"dense_nodes_1\"], activation='relu'))\n",
    "    model.add(Dropout(params['dense_layers']['dense_do_1']))\n",
    "\n",
    "    if params['dense_layers'][\"layers\"] == 'two':\n",
    "        model.add(Dense(params['dense_layers'][\"dense_nodes_2\"], activation='relu', \n",
    "                        kernel_regularizer=params['dense_layers'][\"kernel_regularizer_1\"]))\n",
    "        model.add(Dropout(params['dense_layers']['dense_do_2']))\n",
    "\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    \n",
    "    if params[\"optimizer\"] == 'rmsprop':\n",
    "        optimizer = optimizers.RMSprop(lr=params[\"lr\"])\n",
    "    elif params[\"optimizer\"] == 'sgd':\n",
    "        optimizer = optimizers.SGD(lr=params[\"lr\"], decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    elif params[\"optimizer\"] == 'adam':\n",
    "        optimizer = optimizers.Adam(learning_rate=params[\"lr\"], beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy', f1_metric])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def check_baseline(pred, y_test):\n",
    "    print(\"size of test set\", len(y_test))\n",
    "    e = np.equal(pred, y_test)\n",
    "    print(\"TP class counts\", np.unique(y_test[e], return_counts=True))\n",
    "    print(\"True class counts\", np.unique(y_test, return_counts=True))\n",
    "    print(\"Pred class counts\", np.unique(pred, return_counts=True))\n",
    "    holds = np.unique(y_test, return_counts=True)[1][2]  # number 'hold' predictions\n",
    "    print(\"baseline acc:\", (holds/len(y_test)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from IPython.display import SVG\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "model = create_model_cnn(params)\n",
    "\n",
    "best_model_path = os.path.join('.', 'best_model_keras')\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,\n",
    "                   patience=100, min_delta=0.0001)\n",
    "# csv_logger = CSVLogger(os.path.join(OUTPUT_PATH, 'log_training_batch.log'), append=True)\n",
    "rlp = ReduceLROnPlateau(monitor='val_loss', factor=0.02, patience=20, verbose=1, mode='min',\n",
    "                        min_delta=0.001, cooldown=1, min_lr=0.0001)\n",
    "mcp = ModelCheckpoint(best_model_path, monitor='val_f1_metric', verbose=1,\n",
    "                      save_best_only=True, save_weights_only=False, mode='max', period=1)  # val_f1_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "30/30 [==============================] - 4s 75ms/step - loss: 1.0312 - accuracy: 0.2981 - f1_metric: 0.1429 - val_loss: 1.0795 - val_accuracy: 0.1811 - val_f1_metric: 0.1061\n",
      "\n",
      "Epoch 00001: val_f1_metric improved from -inf to 0.10608, saving model to ./best_model_keras\n",
      "INFO:tensorflow:Assets written to: ./best_model_keras/assets\n",
      "Epoch 2/200\n",
      "30/30 [==============================] - 2s 64ms/step - loss: 0.7899 - accuracy: 0.3133 - f1_metric: 0.1987 - val_loss: 1.1389 - val_accuracy: 0.1747 - val_f1_metric: 0.1289\n",
      "\n",
      "Epoch 00002: val_f1_metric improved from 0.10608 to 0.12895, saving model to ./best_model_keras\n",
      "INFO:tensorflow:Assets written to: ./best_model_keras/assets\n",
      "Epoch 3/200\n",
      "30/30 [==============================] - 2s 58ms/step - loss: 0.7046 - accuracy: 0.3565 - f1_metric: 0.2537 - val_loss: 1.0405 - val_accuracy: 0.3200 - val_f1_metric: 0.2509\n",
      "\n",
      "Epoch 00003: val_f1_metric improved from 0.12895 to 0.25085, saving model to ./best_model_keras\n",
      "INFO:tensorflow:Assets written to: ./best_model_keras/assets\n",
      "Epoch 4/200\n",
      "30/30 [==============================] - 2s 59ms/step - loss: 0.6727 - accuracy: 0.3939 - f1_metric: 0.3314 - val_loss: 0.9736 - val_accuracy: 0.3011 - val_f1_metric: 0.1655\n",
      "\n",
      "Epoch 00004: val_f1_metric did not improve from 0.25085\n",
      "Epoch 5/200\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 0.6679 - accuracy: 0.4034 - f1_metric: 0.3485 - val_loss: 0.8629 - val_accuracy: 0.4716 - val_f1_metric: 0.3984\n",
      "\n",
      "Epoch 00005: val_f1_metric improved from 0.25085 to 0.39838, saving model to ./best_model_keras\n",
      "INFO:tensorflow:Assets written to: ./best_model_keras/assets\n",
      "Epoch 6/200\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 0.6666 - accuracy: 0.3565 - f1_metric: 0.2636 - val_loss: 0.9709 - val_accuracy: 0.3916 - val_f1_metric: 0.3766\n",
      "\n",
      "Epoch 00006: val_f1_metric did not improve from 0.39838\n",
      "Epoch 7/200\n",
      "30/30 [==============================] - 2s 68ms/step - loss: 0.6057 - accuracy: 0.4961 - f1_metric: 0.4794 - val_loss: 1.1244 - val_accuracy: 0.2589 - val_f1_metric: 0.1705\n",
      "\n",
      "Epoch 00007: val_f1_metric did not improve from 0.39838\n",
      "Epoch 8/200\n",
      "30/30 [==============================] - 2s 69ms/step - loss: 0.6190 - accuracy: 0.4034 - f1_metric: 0.3674 - val_loss: 0.9755 - val_accuracy: 0.3832 - val_f1_metric: 0.3636\n",
      "\n",
      "Epoch 00008: val_f1_metric did not improve from 0.39838\n",
      "Epoch 9/200\n",
      "30/30 [==============================] - 2s 73ms/step - loss: 0.5951 - accuracy: 0.4444 - f1_metric: 0.4308 - val_loss: 0.9621 - val_accuracy: 0.3789 - val_f1_metric: 0.3616\n",
      "\n",
      "Epoch 00009: val_f1_metric did not improve from 0.39838\n",
      "Epoch 10/200\n",
      "30/30 [==============================] - 2s 70ms/step - loss: 0.5809 - accuracy: 0.4529 - f1_metric: 0.4447 - val_loss: 0.9161 - val_accuracy: 0.4189 - val_f1_metric: 0.4022\n",
      "\n",
      "Epoch 00010: val_f1_metric improved from 0.39838 to 0.40225, saving model to ./best_model_keras\n",
      "INFO:tensorflow:Assets written to: ./best_model_keras/assets\n",
      "Epoch 11/200\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 0.5919 - accuracy: 0.4602 - f1_metric: 0.4527 - val_loss: 0.8084 - val_accuracy: 0.4505 - val_f1_metric: 0.4359\n",
      "\n",
      "Epoch 00011: val_f1_metric improved from 0.40225 to 0.43594, saving model to ./best_model_keras\n",
      "INFO:tensorflow:Assets written to: ./best_model_keras/assets\n",
      "Epoch 12/200\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 0.5738 - accuracy: 0.4776 - f1_metric: 0.4667 - val_loss: 0.8282 - val_accuracy: 0.4821 - val_f1_metric: 0.4807\n",
      "\n",
      "Epoch 00012: val_f1_metric improved from 0.43594 to 0.48067, saving model to ./best_model_keras\n",
      "INFO:tensorflow:Assets written to: ./best_model_keras/assets\n",
      "Epoch 13/200\n",
      "30/30 [==============================] - 2s 61ms/step - loss: 0.5818 - accuracy: 0.4887 - f1_metric: 0.4872 - val_loss: 0.7806 - val_accuracy: 0.4821 - val_f1_metric: 0.4650\n",
      "\n",
      "Epoch 00013: val_f1_metric did not improve from 0.48067\n",
      "Epoch 14/200\n",
      "30/30 [==============================] - 2s 63ms/step - loss: 0.5648 - accuracy: 0.4866 - f1_metric: 0.4771 - val_loss: 0.8295 - val_accuracy: 0.4926 - val_f1_metric: 0.4835\n",
      "\n",
      "Epoch 00014: val_f1_metric improved from 0.48067 to 0.48353, saving model to ./best_model_keras\n",
      "INFO:tensorflow:Assets written to: ./best_model_keras/assets\n",
      "Epoch 15/200\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 0.5579 - accuracy: 0.4650 - f1_metric: 0.4578 - val_loss: 0.7780 - val_accuracy: 0.5011 - val_f1_metric: 0.4985\n",
      "\n",
      "Epoch 00015: val_f1_metric improved from 0.48353 to 0.49847, saving model to ./best_model_keras\n",
      "INFO:tensorflow:Assets written to: ./best_model_keras/assets\n",
      "Epoch 16/200\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 0.5495 - accuracy: 0.5134 - f1_metric: 0.5079 - val_loss: 0.8194 - val_accuracy: 0.4758 - val_f1_metric: 0.4714\n",
      "\n",
      "Epoch 00016: val_f1_metric did not improve from 0.49847\n",
      "Epoch 17/200\n",
      "30/30 [==============================] - 2s 76ms/step - loss: 0.5245 - accuracy: 0.5303 - f1_metric: 0.5268 - val_loss: 0.7697 - val_accuracy: 0.5305 - val_f1_metric: 0.5270\n",
      "\n",
      "Epoch 00017: val_f1_metric improved from 0.49847 to 0.52698, saving model to ./best_model_keras\n",
      "INFO:tensorflow:Assets written to: ./best_model_keras/assets\n",
      "Epoch 18/200\n",
      "30/30 [==============================] - 2s 59ms/step - loss: 0.5217 - accuracy: 0.5219 - f1_metric: 0.5201 - val_loss: 0.7307 - val_accuracy: 0.5684 - val_f1_metric: 0.5633\n",
      "\n",
      "Epoch 00018: val_f1_metric improved from 0.52698 to 0.56328, saving model to ./best_model_keras\n",
      "INFO:tensorflow:Assets written to: ./best_model_keras/assets\n",
      "Epoch 19/200\n",
      "30/30 [==============================] - 2s 65ms/step - loss: 0.5175 - accuracy: 0.5176 - f1_metric: 0.5168 - val_loss: 0.8091 - val_accuracy: 0.5116 - val_f1_metric: 0.5053\n",
      "\n",
      "Epoch 00019: val_f1_metric did not improve from 0.56328\n",
      "Epoch 20/200\n",
      "30/30 [==============================] - 2s 64ms/step - loss: 0.5286 - accuracy: 0.5108 - f1_metric: 0.5092 - val_loss: 0.7319 - val_accuracy: 0.5537 - val_f1_metric: 0.5456\n",
      "\n",
      "Epoch 00020: val_f1_metric did not improve from 0.56328\n",
      "Epoch 21/200\n",
      "30/30 [==============================] - 2s 67ms/step - loss: 0.5160 - accuracy: 0.5076 - f1_metric: 0.5071 - val_loss: 0.8295 - val_accuracy: 0.4947 - val_f1_metric: 0.4925\n",
      "\n",
      "Epoch 00021: val_f1_metric did not improve from 0.56328\n",
      "Epoch 22/200\n",
      "30/30 [==============================] - 2s 59ms/step - loss: 0.5107 - accuracy: 0.5261 - f1_metric: 0.5257 - val_loss: 0.7267 - val_accuracy: 0.5705 - val_f1_metric: 0.5650\n",
      "\n",
      "Epoch 00022: val_f1_metric improved from 0.56328 to 0.56498, saving model to ./best_model_keras\n",
      "INFO:tensorflow:Assets written to: ./best_model_keras/assets\n",
      "Epoch 23/200\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 0.5078 - accuracy: 0.5403 - f1_metric: 0.5388 - val_loss: 0.8293 - val_accuracy: 0.4463 - val_f1_metric: 0.4402\n",
      "\n",
      "Epoch 00023: val_f1_metric did not improve from 0.56498\n",
      "Epoch 24/200\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 0.5002 - accuracy: 0.5266 - f1_metric: 0.5241 - val_loss: 0.7758 - val_accuracy: 0.5053 - val_f1_metric: 0.5085\n",
      "\n",
      "Epoch 00024: val_f1_metric did not improve from 0.56498\n",
      "Epoch 25/200\n",
      "30/30 [==============================] - 2s 58ms/step - loss: 0.4841 - accuracy: 0.5587 - f1_metric: 0.5579 - val_loss: 0.8010 - val_accuracy: 0.5137 - val_f1_metric: 0.5126\n",
      "\n",
      "Epoch 00025: val_f1_metric did not improve from 0.56498\n",
      "Epoch 26/200\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 0.4870 - accuracy: 0.5492 - f1_metric: 0.5477 - val_loss: 0.9075 - val_accuracy: 0.4526 - val_f1_metric: 0.4537\n",
      "\n",
      "Epoch 00026: val_f1_metric did not improve from 0.56498\n",
      "Epoch 27/200\n",
      "30/30 [==============================] - 2s 59ms/step - loss: 0.4732 - accuracy: 0.5361 - f1_metric: 0.5347 - val_loss: 0.7859 - val_accuracy: 0.5326 - val_f1_metric: 0.5348\n",
      "\n",
      "Epoch 00027: val_f1_metric did not improve from 0.56498\n",
      "Epoch 28/200\n",
      "30/30 [==============================] - 2s 61ms/step - loss: 0.4771 - accuracy: 0.5498 - f1_metric: 0.5498 - val_loss: 0.6730 - val_accuracy: 0.5789 - val_f1_metric: 0.5778\n",
      "\n",
      "Epoch 00028: val_f1_metric improved from 0.56498 to 0.57784, saving model to ./best_model_keras\n",
      "INFO:tensorflow:Assets written to: ./best_model_keras/assets\n",
      "Epoch 29/200\n",
      "30/30 [==============================] - 1s 50ms/step - loss: 0.4978 - accuracy: 0.5471 - f1_metric: 0.5471 - val_loss: 0.7749 - val_accuracy: 0.5305 - val_f1_metric: 0.5306\n",
      "\n",
      "Epoch 00029: val_f1_metric did not improve from 0.57784\n",
      "Epoch 30/200\n",
      "30/30 [==============================] - 1s 50ms/step - loss: 0.4764 - accuracy: 0.5434 - f1_metric: 0.5440 - val_loss: 0.6422 - val_accuracy: 0.5979 - val_f1_metric: 0.5975\n",
      "\n",
      "Epoch 00030: val_f1_metric improved from 0.57784 to 0.59751, saving model to ./best_model_keras\n",
      "INFO:tensorflow:Assets written to: ./best_model_keras/assets\n",
      "Epoch 31/200\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.4852 - accuracy: 0.5324 - f1_metric: 0.5319 - val_loss: 0.6958 - val_accuracy: 0.5474 - val_f1_metric: 0.5485\n",
      "\n",
      "Epoch 00031: val_f1_metric did not improve from 0.59751\n",
      "Epoch 32/200\n",
      "30/30 [==============================] - 2s 50ms/step - loss: 0.4703 - accuracy: 0.5735 - f1_metric: 0.5745 - val_loss: 0.6952 - val_accuracy: 0.5558 - val_f1_metric: 0.5558\n",
      "\n",
      "Epoch 00032: val_f1_metric did not improve from 0.59751\n",
      "Epoch 33/200\n",
      "30/30 [==============================] - 2s 50ms/step - loss: 0.4789 - accuracy: 0.5166 - f1_metric: 0.5166 - val_loss: 0.6923 - val_accuracy: 0.5579 - val_f1_metric: 0.5577\n",
      "\n",
      "Epoch 00033: val_f1_metric did not improve from 0.59751\n",
      "Epoch 34/200\n",
      "30/30 [==============================] - 2s 52ms/step - loss: 0.4682 - accuracy: 0.5814 - f1_metric: 0.5818 - val_loss: 0.5879 - val_accuracy: 0.6400 - val_f1_metric: 0.6411\n",
      "\n",
      "Epoch 00034: val_f1_metric improved from 0.59751 to 0.64108, saving model to ./best_model_keras\n",
      "INFO:tensorflow:Assets written to: ./best_model_keras/assets\n",
      "Epoch 35/200\n",
      "30/30 [==============================] - 1s 47ms/step - loss: 0.4802 - accuracy: 0.5824 - f1_metric: 0.5835 - val_loss: 0.8325 - val_accuracy: 0.4779 - val_f1_metric: 0.4797\n",
      "\n",
      "Epoch 00035: val_f1_metric did not improve from 0.64108\n",
      "Epoch 36/200\n",
      "30/30 [==============================] - 2s 52ms/step - loss: 0.4580 - accuracy: 0.5756 - f1_metric: 0.5754 - val_loss: 0.8829 - val_accuracy: 0.4905 - val_f1_metric: 0.4911\n",
      "\n",
      "Epoch 00036: val_f1_metric did not improve from 0.64108\n",
      "Epoch 37/200\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 0.4599 - accuracy: 0.5545 - f1_metric: 0.5553 - val_loss: 0.7756 - val_accuracy: 0.5453 - val_f1_metric: 0.5433\n",
      "\n",
      "Epoch 00037: val_f1_metric did not improve from 0.64108\n",
      "Epoch 38/200\n",
      "30/30 [==============================] - 1s 49ms/step - loss: 0.4578 - accuracy: 0.5756 - f1_metric: 0.5748 - val_loss: 0.7286 - val_accuracy: 0.5432 - val_f1_metric: 0.5404\n",
      "\n",
      "Epoch 00038: val_f1_metric did not improve from 0.64108\n",
      "Epoch 39/200\n",
      "30/30 [==============================] - 2s 51ms/step - loss: 0.4306 - accuracy: 0.5945 - f1_metric: 0.5955 - val_loss: 0.6816 - val_accuracy: 0.5811 - val_f1_metric: 0.5819\n",
      "\n",
      "Epoch 00039: val_f1_metric did not improve from 0.64108\n",
      "Epoch 40/200\n",
      "30/30 [==============================] - 1s 49ms/step - loss: 0.4595 - accuracy: 0.5908 - f1_metric: 0.5890 - val_loss: 0.7160 - val_accuracy: 0.5242 - val_f1_metric: 0.5250\n",
      "\n",
      "Epoch 00040: val_f1_metric did not improve from 0.64108\n",
      "Epoch 41/200\n",
      "30/30 [==============================] - 2s 53ms/step - loss: 0.4615 - accuracy: 0.5482 - f1_metric: 0.5451 - val_loss: 0.7510 - val_accuracy: 0.5537 - val_f1_metric: 0.5511\n",
      "\n",
      "Epoch 00041: val_f1_metric did not improve from 0.64108\n",
      "Epoch 42/200\n",
      "30/30 [==============================] - 2s 52ms/step - loss: 0.4325 - accuracy: 0.6182 - f1_metric: 0.6181 - val_loss: 0.7905 - val_accuracy: 0.5032 - val_f1_metric: 0.5069\n",
      "\n",
      "Epoch 00042: val_f1_metric did not improve from 0.64108\n",
      "Epoch 43/200\n",
      "30/30 [==============================] - 1s 49ms/step - loss: 0.4412 - accuracy: 0.5635 - f1_metric: 0.5647 - val_loss: 0.7927 - val_accuracy: 0.5158 - val_f1_metric: 0.5187\n",
      "\n",
      "Epoch 00043: val_f1_metric did not improve from 0.64108\n",
      "Epoch 44/200\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 0.4460 - accuracy: 0.5682 - f1_metric: 0.5690 - val_loss: 0.7632 - val_accuracy: 0.5368 - val_f1_metric: 0.5355\n",
      "\n",
      "Epoch 00044: val_f1_metric did not improve from 0.64108\n",
      "Epoch 45/200\n",
      "30/30 [==============================] - 2s 53ms/step - loss: 0.4121 - accuracy: 0.6077 - f1_metric: 0.6068 - val_loss: 0.7699 - val_accuracy: 0.5537 - val_f1_metric: 0.5538\n",
      "\n",
      "Epoch 00045: val_f1_metric did not improve from 0.64108\n",
      "Epoch 46/200\n",
      "30/30 [==============================] - 2s 52ms/step - loss: 0.4202 - accuracy: 0.6309 - f1_metric: 0.6306 - val_loss: 0.8211 - val_accuracy: 0.5474 - val_f1_metric: 0.5453\n",
      "\n",
      "Epoch 00046: val_f1_metric did not improve from 0.64108\n",
      "Epoch 47/200\n",
      "30/30 [==============================] - 2s 50ms/step - loss: 0.4101 - accuracy: 0.6077 - f1_metric: 0.6070 - val_loss: 0.8256 - val_accuracy: 0.5411 - val_f1_metric: 0.5394\n",
      "\n",
      "Epoch 00047: val_f1_metric did not improve from 0.64108\n",
      "Epoch 48/200\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 0.4272 - accuracy: 0.5987 - f1_metric: 0.5980 - val_loss: 0.7448 - val_accuracy: 0.5558 - val_f1_metric: 0.5531\n",
      "\n",
      "Epoch 00048: val_f1_metric did not improve from 0.64108\n",
      "Epoch 49/200\n",
      "30/30 [==============================] - 2s 52ms/step - loss: 0.4241 - accuracy: 0.6203 - f1_metric: 0.6207 - val_loss: 0.7576 - val_accuracy: 0.5432 - val_f1_metric: 0.5386\n",
      "\n",
      "Epoch 00049: val_f1_metric did not improve from 0.64108\n",
      "Epoch 50/200\n",
      "30/30 [==============================] - 2s 50ms/step - loss: 0.4212 - accuracy: 0.5793 - f1_metric: 0.5803 - val_loss: 0.6888 - val_accuracy: 0.5726 - val_f1_metric: 0.5741\n",
      "\n",
      "Epoch 00050: val_f1_metric did not improve from 0.64108\n",
      "Epoch 51/200\n",
      "30/30 [==============================] - 1s 49ms/step - loss: 0.4201 - accuracy: 0.6151 - f1_metric: 0.6149 - val_loss: 0.6528 - val_accuracy: 0.5811 - val_f1_metric: 0.5792\n",
      "\n",
      "Epoch 00051: val_f1_metric did not improve from 0.64108\n",
      "Epoch 52/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 0.3919 - accuracy: 0.6361 - f1_metric: 0.6362 - val_loss: 0.6545 - val_accuracy: 0.6105 - val_f1_metric: 0.6066\n",
      "\n",
      "Epoch 00052: val_f1_metric did not improve from 0.64108\n",
      "Epoch 53/200\n",
      "30/30 [==============================] - 2s 50ms/step - loss: 0.3898 - accuracy: 0.6335 - f1_metric: 0.6337 - val_loss: 0.6879 - val_accuracy: 0.5916 - val_f1_metric: 0.5943\n",
      "\n",
      "Epoch 00053: val_f1_metric did not improve from 0.64108\n",
      "Epoch 54/200\n",
      "30/30 [==============================] - 2s 50ms/step - loss: 0.4102 - accuracy: 0.6303 - f1_metric: 0.6293 - val_loss: 0.6719 - val_accuracy: 0.5705 - val_f1_metric: 0.5748\n",
      "\n",
      "Epoch 00054: val_f1_metric did not improve from 0.64108\n",
      "\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "Epoch 55/200\n",
      "30/30 [==============================] - 2s 51ms/step - loss: 0.3832 - accuracy: 0.6224 - f1_metric: 0.6222 - val_loss: 0.6703 - val_accuracy: 0.5747 - val_f1_metric: 0.5760\n",
      "\n",
      "Epoch 00055: val_f1_metric did not improve from 0.64108\n",
      "Epoch 56/200\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 0.3867 - accuracy: 0.6345 - f1_metric: 0.6357 - val_loss: 0.6905 - val_accuracy: 0.5705 - val_f1_metric: 0.5721\n",
      "\n",
      "Epoch 00056: val_f1_metric did not improve from 0.64108\n",
      "Epoch 57/200\n",
      "30/30 [==============================] - 2s 51ms/step - loss: 0.3775 - accuracy: 0.6219 - f1_metric: 0.6224 - val_loss: 0.6799 - val_accuracy: 0.5832 - val_f1_metric: 0.5838\n",
      "\n",
      "Epoch 00057: val_f1_metric did not improve from 0.64108\n",
      "Epoch 58/200\n",
      "30/30 [==============================] - 2s 51ms/step - loss: 0.3760 - accuracy: 0.6440 - f1_metric: 0.6436 - val_loss: 0.6872 - val_accuracy: 0.5832 - val_f1_metric: 0.5838\n",
      "\n",
      "Epoch 00058: val_f1_metric did not improve from 0.64108\n",
      "Epoch 59/200\n",
      "30/30 [==============================] - 2s 53ms/step - loss: 0.3705 - accuracy: 0.6403 - f1_metric: 0.6412 - val_loss: 0.6651 - val_accuracy: 0.5958 - val_f1_metric: 0.5956\n",
      "\n",
      "Epoch 00059: val_f1_metric did not improve from 0.64108\n",
      "Epoch 60/200\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 0.3518 - accuracy: 0.6461 - f1_metric: 0.6467 - val_loss: 0.6379 - val_accuracy: 0.6147 - val_f1_metric: 0.6131\n",
      "\n",
      "Epoch 00060: val_f1_metric did not improve from 0.64108\n",
      "Epoch 61/200\n",
      "30/30 [==============================] - 2s 50ms/step - loss: 0.3541 - accuracy: 0.6651 - f1_metric: 0.6667 - val_loss: 0.6519 - val_accuracy: 0.6126 - val_f1_metric: 0.6112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00061: val_f1_metric did not improve from 0.64108\n",
      "Epoch 62/200\n",
      "30/30 [==============================] - 2s 52ms/step - loss: 0.3560 - accuracy: 0.6546 - f1_metric: 0.6553 - val_loss: 0.6607 - val_accuracy: 0.6042 - val_f1_metric: 0.6034\n",
      "\n",
      "Epoch 00062: val_f1_metric did not improve from 0.64108\n",
      "Epoch 63/200\n",
      "30/30 [==============================] - 2s 51ms/step - loss: 0.3526 - accuracy: 0.6556 - f1_metric: 0.6558 - val_loss: 0.6407 - val_accuracy: 0.6211 - val_f1_metric: 0.6190\n",
      "\n",
      "Epoch 00063: val_f1_metric did not improve from 0.64108\n",
      "Epoch 64/200\n",
      "30/30 [==============================] - 2s 53ms/step - loss: 0.3506 - accuracy: 0.6856 - f1_metric: 0.6858 - val_loss: 0.6347 - val_accuracy: 0.6253 - val_f1_metric: 0.6229\n",
      "\n",
      "Epoch 00064: val_f1_metric did not improve from 0.64108\n",
      "Epoch 65/200\n",
      "30/30 [==============================] - 2s 52ms/step - loss: 0.3530 - accuracy: 0.6709 - f1_metric: 0.6702 - val_loss: 0.6411 - val_accuracy: 0.6316 - val_f1_metric: 0.6288\n",
      "\n",
      "Epoch 00065: val_f1_metric did not improve from 0.64108\n",
      "Epoch 66/200\n",
      "30/30 [==============================] - 1s 49ms/step - loss: 0.3422 - accuracy: 0.6693 - f1_metric: 0.6696 - val_loss: 0.6285 - val_accuracy: 0.6358 - val_f1_metric: 0.6327\n",
      "\n",
      "Epoch 00066: val_f1_metric did not improve from 0.64108\n",
      "Epoch 67/200\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 0.3572 - accuracy: 0.6730 - f1_metric: 0.6738 - val_loss: 0.6395 - val_accuracy: 0.6232 - val_f1_metric: 0.6236\n",
      "\n",
      "Epoch 00067: val_f1_metric did not improve from 0.64108\n",
      "Epoch 68/200\n",
      "30/30 [==============================] - 1s 49ms/step - loss: 0.3422 - accuracy: 0.6756 - f1_metric: 0.6764 - val_loss: 0.6345 - val_accuracy: 0.6295 - val_f1_metric: 0.6268\n",
      "\n",
      "Epoch 00068: val_f1_metric did not improve from 0.64108\n",
      "Epoch 69/200\n",
      "30/30 [==============================] - 2s 52ms/step - loss: 0.3405 - accuracy: 0.6746 - f1_metric: 0.6756 - val_loss: 0.6271 - val_accuracy: 0.6274 - val_f1_metric: 0.6249\n",
      "\n",
      "Epoch 00069: val_f1_metric did not improve from 0.64108\n",
      "Epoch 70/200\n",
      "30/30 [==============================] - 1s 49ms/step - loss: 0.3494 - accuracy: 0.6798 - f1_metric: 0.6798 - val_loss: 0.6529 - val_accuracy: 0.6189 - val_f1_metric: 0.6170\n",
      "\n",
      "Epoch 00070: val_f1_metric did not improve from 0.64108\n",
      "Epoch 71/200\n",
      "30/30 [==============================] - 2s 51ms/step - loss: 0.3414 - accuracy: 0.6761 - f1_metric: 0.6743 - val_loss: 0.6283 - val_accuracy: 0.6295 - val_f1_metric: 0.6268\n",
      "\n",
      "Epoch 00071: val_f1_metric did not improve from 0.64108\n",
      "Epoch 72/200\n",
      "30/30 [==============================] - 2s 50ms/step - loss: 0.3541 - accuracy: 0.6761 - f1_metric: 0.6766 - val_loss: 0.6355 - val_accuracy: 0.6316 - val_f1_metric: 0.6288\n",
      "\n",
      "Epoch 00072: val_f1_metric did not improve from 0.64108\n",
      "Epoch 73/200\n",
      "30/30 [==============================] - 2s 52ms/step - loss: 0.3384 - accuracy: 0.6767 - f1_metric: 0.6766 - val_loss: 0.6380 - val_accuracy: 0.6274 - val_f1_metric: 0.6275\n",
      "\n",
      "Epoch 00073: val_f1_metric did not improve from 0.64108\n",
      "Epoch 74/200\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.3476 - accuracy: 0.6725 - f1_metric: 0.6727 - val_loss: 0.6375 - val_accuracy: 0.6337 - val_f1_metric: 0.6334\n",
      "\n",
      "Epoch 00074: val_f1_metric did not improve from 0.64108\n",
      "Epoch 75/200\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 0.3468 - accuracy: 0.6646 - f1_metric: 0.6657 - val_loss: 0.6399 - val_accuracy: 0.6316 - val_f1_metric: 0.6314\n",
      "\n",
      "Epoch 00075: val_f1_metric did not improve from 0.64108\n",
      "Epoch 76/200\n",
      "30/30 [==============================] - 2s 50ms/step - loss: 0.3310 - accuracy: 0.6925 - f1_metric: 0.6928 - val_loss: 0.6365 - val_accuracy: 0.6337 - val_f1_metric: 0.6334\n",
      "\n",
      "Epoch 00076: val_f1_metric did not improve from 0.64108\n",
      "Epoch 77/200\n",
      "30/30 [==============================] - 2s 50ms/step - loss: 0.3579 - accuracy: 0.6725 - f1_metric: 0.6704 - val_loss: 0.6659 - val_accuracy: 0.6105 - val_f1_metric: 0.6092\n",
      "\n",
      "Epoch 00077: val_f1_metric did not improve from 0.64108\n",
      "Epoch 78/200\n",
      "30/30 [==============================] - 2s 50ms/step - loss: 0.3318 - accuracy: 0.6667 - f1_metric: 0.6670 - val_loss: 0.6407 - val_accuracy: 0.6316 - val_f1_metric: 0.6288\n",
      "\n",
      "Epoch 00078: val_f1_metric did not improve from 0.64108\n",
      "Epoch 79/200\n",
      "30/30 [==============================] - 2s 52ms/step - loss: 0.3301 - accuracy: 0.6840 - f1_metric: 0.6842 - val_loss: 0.6187 - val_accuracy: 0.6358 - val_f1_metric: 0.6327\n",
      "\n",
      "Epoch 00079: val_f1_metric did not improve from 0.64108\n",
      "Epoch 80/200\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.3471 - accuracy: 0.6846 - f1_metric: 0.6842 - val_loss: 0.6704 - val_accuracy: 0.6105 - val_f1_metric: 0.6092\n",
      "\n",
      "Epoch 00080: val_f1_metric did not improve from 0.64108\n",
      "Epoch 81/200\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 0.3338 - accuracy: 0.6788 - f1_metric: 0.6782 - val_loss: 0.6258 - val_accuracy: 0.6295 - val_f1_metric: 0.6268\n",
      "\n",
      "Epoch 00081: val_f1_metric did not improve from 0.64108\n",
      "Epoch 82/200\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 0.3554 - accuracy: 0.6677 - f1_metric: 0.6683 - val_loss: 0.6549 - val_accuracy: 0.6105 - val_f1_metric: 0.6092\n",
      "\n",
      "Epoch 00082: val_f1_metric did not improve from 0.64108\n",
      "Epoch 83/200\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 0.3346 - accuracy: 0.6756 - f1_metric: 0.6766 - val_loss: 0.6302 - val_accuracy: 0.6295 - val_f1_metric: 0.6295\n",
      "\n",
      "Epoch 00083: val_f1_metric did not improve from 0.64108\n",
      "Epoch 84/200\n",
      "30/30 [==============================] - 1s 49ms/step - loss: 0.3475 - accuracy: 0.6877 - f1_metric: 0.6863 - val_loss: 0.6471 - val_accuracy: 0.6189 - val_f1_metric: 0.6197\n",
      "\n",
      "Epoch 00084: val_f1_metric did not improve from 0.64108\n",
      "Epoch 85/200\n",
      "30/30 [==============================] - 2s 52ms/step - loss: 0.3273 - accuracy: 0.6761 - f1_metric: 0.6746 - val_loss: 0.6207 - val_accuracy: 0.6316 - val_f1_metric: 0.6314\n",
      "\n",
      "Epoch 00085: val_f1_metric did not improve from 0.64108\n",
      "Epoch 86/200\n",
      "30/30 [==============================] - 2s 53ms/step - loss: 0.3399 - accuracy: 0.6777 - f1_metric: 0.6766 - val_loss: 0.6340 - val_accuracy: 0.6316 - val_f1_metric: 0.6314\n",
      "\n",
      "Epoch 00086: val_f1_metric did not improve from 0.64108\n",
      "Epoch 87/200\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 0.3387 - accuracy: 0.6856 - f1_metric: 0.6847 - val_loss: 0.6264 - val_accuracy: 0.6316 - val_f1_metric: 0.6314\n",
      "\n",
      "Epoch 00087: val_f1_metric did not improve from 0.64108\n",
      "Epoch 88/200\n",
      "30/30 [==============================] - 2s 52ms/step - loss: 0.3259 - accuracy: 0.6909 - f1_metric: 0.6927 - val_loss: 0.6246 - val_accuracy: 0.6400 - val_f1_metric: 0.6393\n",
      "\n",
      "Epoch 00088: val_f1_metric did not improve from 0.64108\n",
      "Epoch 89/200\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 0.3355 - accuracy: 0.6840 - f1_metric: 0.6852 - val_loss: 0.6290 - val_accuracy: 0.6358 - val_f1_metric: 0.6353\n",
      "\n",
      "Epoch 00089: val_f1_metric did not improve from 0.64108\n",
      "Epoch 90/200\n",
      "30/30 [==============================] - 2s 52ms/step - loss: 0.3465 - accuracy: 0.6840 - f1_metric: 0.6839 - val_loss: 0.6381 - val_accuracy: 0.6274 - val_f1_metric: 0.6275\n",
      "\n",
      "Epoch 00090: val_f1_metric did not improve from 0.64108\n",
      "Epoch 91/200\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 0.3327 - accuracy: 0.6872 - f1_metric: 0.6873 - val_loss: 0.6118 - val_accuracy: 0.6379 - val_f1_metric: 0.6400\n",
      "\n",
      "Epoch 00091: val_f1_metric did not improve from 0.64108\n",
      "Epoch 92/200\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 0.3339 - accuracy: 0.6988 - f1_metric: 0.6980 - val_loss: 0.6432 - val_accuracy: 0.6274 - val_f1_metric: 0.6275\n",
      "\n",
      "Epoch 00092: val_f1_metric did not improve from 0.64108\n",
      "Epoch 93/200\n",
      "30/30 [==============================] - 1s 49ms/step - loss: 0.3311 - accuracy: 0.6825 - f1_metric: 0.6826 - val_loss: 0.6389 - val_accuracy: 0.6337 - val_f1_metric: 0.6334\n",
      "\n",
      "Epoch 00093: val_f1_metric did not improve from 0.64108\n",
      "Epoch 94/200\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 0.3284 - accuracy: 0.6835 - f1_metric: 0.6842 - val_loss: 0.6244 - val_accuracy: 0.6442 - val_f1_metric: 0.6432\n",
      "\n",
      "Epoch 00094: val_f1_metric improved from 0.64108 to 0.64316, saving model to ./best_model_keras\n",
      "INFO:tensorflow:Assets written to: ./best_model_keras/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/200\n",
      "30/30 [==============================] - 2s 59ms/step - loss: 0.3424 - accuracy: 0.6714 - f1_metric: 0.6719 - val_loss: 0.6452 - val_accuracy: 0.6253 - val_f1_metric: 0.6256\n",
      "\n",
      "Epoch 00095: val_f1_metric did not improve from 0.64316\n",
      "Epoch 96/200\n",
      "30/30 [==============================] - 3s 113ms/step - loss: 0.3189 - accuracy: 0.7083 - f1_metric: 0.7079 - val_loss: 0.5788 - val_accuracy: 0.6653 - val_f1_metric: 0.6654\n",
      "\n",
      "Epoch 00096: val_f1_metric improved from 0.64316 to 0.66536, saving model to ./best_model_keras\n",
      "INFO:tensorflow:Assets written to: ./best_model_keras/assets\n",
      "Epoch 97/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 0.3373 - accuracy: 0.6909 - f1_metric: 0.6910 - val_loss: 0.6766 - val_accuracy: 0.6211 - val_f1_metric: 0.6190\n",
      "\n",
      "Epoch 00097: val_f1_metric did not improve from 0.66536\n",
      "Epoch 98/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 0.3245 - accuracy: 0.6867 - f1_metric: 0.6876 - val_loss: 0.6327 - val_accuracy: 0.6442 - val_f1_metric: 0.6432\n",
      "\n",
      "Epoch 00098: val_f1_metric did not improve from 0.66536\n",
      "Epoch 99/200\n",
      "30/30 [==============================] - 2s 51ms/step - loss: 0.3174 - accuracy: 0.7041 - f1_metric: 0.7045 - val_loss: 0.6133 - val_accuracy: 0.6463 - val_f1_metric: 0.6451\n",
      "\n",
      "Epoch 00099: val_f1_metric did not improve from 0.66536\n",
      "Epoch 100/200\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 0.3257 - accuracy: 0.7025 - f1_metric: 0.7017 - val_loss: 0.6305 - val_accuracy: 0.6400 - val_f1_metric: 0.6393\n",
      "\n",
      "Epoch 00100: val_f1_metric did not improve from 0.66536\n",
      "Epoch 101/200\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 0.3341 - accuracy: 0.6867 - f1_metric: 0.6855 - val_loss: 0.6135 - val_accuracy: 0.6484 - val_f1_metric: 0.6471\n",
      "\n",
      "Epoch 00101: val_f1_metric did not improve from 0.66536\n",
      "Epoch 102/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 0.3390 - accuracy: 0.6825 - f1_metric: 0.6819 - val_loss: 0.6472 - val_accuracy: 0.6232 - val_f1_metric: 0.6236\n",
      "\n",
      "Epoch 00102: val_f1_metric did not improve from 0.66536\n",
      "Epoch 103/200\n",
      "30/30 [==============================] - 1s 49ms/step - loss: 0.3239 - accuracy: 0.6962 - f1_metric: 0.6957 - val_loss: 0.6286 - val_accuracy: 0.6358 - val_f1_metric: 0.6353\n",
      "\n",
      "Epoch 00103: val_f1_metric did not improve from 0.66536\n",
      "Epoch 104/200\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 0.3255 - accuracy: 0.7088 - f1_metric: 0.7077 - val_loss: 0.6324 - val_accuracy: 0.6379 - val_f1_metric: 0.6373\n",
      "\n",
      "Epoch 00104: val_f1_metric did not improve from 0.66536\n",
      "Epoch 105/200\n",
      "30/30 [==============================] - 2s 53ms/step - loss: 0.3277 - accuracy: 0.6935 - f1_metric: 0.6941 - val_loss: 0.6281 - val_accuracy: 0.6337 - val_f1_metric: 0.6334\n",
      "\n",
      "Epoch 00105: val_f1_metric did not improve from 0.66536\n",
      "Epoch 106/200\n",
      "30/30 [==============================] - 2s 52ms/step - loss: 0.3289 - accuracy: 0.6888 - f1_metric: 0.6874 - val_loss: 0.6226 - val_accuracy: 0.6400 - val_f1_metric: 0.6393\n",
      "\n",
      "Epoch 00106: val_f1_metric did not improve from 0.66536\n",
      "Epoch 107/200\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 0.3128 - accuracy: 0.6956 - f1_metric: 0.6962 - val_loss: 0.6012 - val_accuracy: 0.6526 - val_f1_metric: 0.6510\n",
      "\n",
      "Epoch 00107: val_f1_metric did not improve from 0.66536\n",
      "Epoch 108/200\n",
      "30/30 [==============================] - 1s 50ms/step - loss: 0.3155 - accuracy: 0.7062 - f1_metric: 0.7056 - val_loss: 0.6210 - val_accuracy: 0.6421 - val_f1_metric: 0.6412\n",
      "\n",
      "Epoch 00108: val_f1_metric did not improve from 0.66536\n",
      "Epoch 109/200\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 0.3239 - accuracy: 0.6930 - f1_metric: 0.6928 - val_loss: 0.6278 - val_accuracy: 0.6442 - val_f1_metric: 0.6432\n",
      "\n",
      "Epoch 00109: val_f1_metric did not improve from 0.66536\n",
      "Epoch 110/200\n",
      "30/30 [==============================] - 2s 52ms/step - loss: 0.3306 - accuracy: 0.6898 - f1_metric: 0.6887 - val_loss: 0.6147 - val_accuracy: 0.6568 - val_f1_metric: 0.6549\n",
      "\n",
      "Epoch 00110: val_f1_metric did not improve from 0.66536\n",
      "Epoch 111/200\n",
      "30/30 [==============================] - 1s 49ms/step - loss: 0.3110 - accuracy: 0.7172 - f1_metric: 0.7183 - val_loss: 0.5996 - val_accuracy: 0.6611 - val_f1_metric: 0.6588\n",
      "\n",
      "Epoch 00111: val_f1_metric did not improve from 0.66536\n",
      "Epoch 112/200\n",
      "30/30 [==============================] - 2s 52ms/step - loss: 0.3146 - accuracy: 0.7014 - f1_metric: 0.7006 - val_loss: 0.6019 - val_accuracy: 0.6632 - val_f1_metric: 0.6634\n",
      "\n",
      "Epoch 00112: val_f1_metric did not improve from 0.66536\n",
      "Epoch 113/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 0.3160 - accuracy: 0.7156 - f1_metric: 0.7165 - val_loss: 0.6156 - val_accuracy: 0.6505 - val_f1_metric: 0.6490\n",
      "\n",
      "Epoch 00113: val_f1_metric did not improve from 0.66536\n",
      "Epoch 114/200\n",
      "30/30 [==============================] - 2s 50ms/step - loss: 0.3064 - accuracy: 0.7030 - f1_metric: 0.7027 - val_loss: 0.6098 - val_accuracy: 0.6463 - val_f1_metric: 0.6451\n",
      "\n",
      "Epoch 00114: val_f1_metric did not improve from 0.66536\n",
      "Epoch 115/200\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 0.3250 - accuracy: 0.6962 - f1_metric: 0.6962 - val_loss: 0.6338 - val_accuracy: 0.6358 - val_f1_metric: 0.6353\n",
      "\n",
      "Epoch 00115: val_f1_metric did not improve from 0.66536\n",
      "Epoch 116/200\n",
      "30/30 [==============================] - 2s 52ms/step - loss: 0.3163 - accuracy: 0.7062 - f1_metric: 0.7068 - val_loss: 0.6247 - val_accuracy: 0.6379 - val_f1_metric: 0.6373\n",
      "\n",
      "Epoch 00116: val_f1_metric did not improve from 0.66536\n",
      "Epoch 117/200\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 0.3183 - accuracy: 0.6967 - f1_metric: 0.6958 - val_loss: 0.6067 - val_accuracy: 0.6526 - val_f1_metric: 0.6510\n",
      "\n",
      "Epoch 00117: val_f1_metric did not improve from 0.66536\n",
      "Epoch 118/200\n",
      "30/30 [==============================] - 2s 53ms/step - loss: 0.3151 - accuracy: 0.7162 - f1_metric: 0.7157 - val_loss: 0.6039 - val_accuracy: 0.6632 - val_f1_metric: 0.6607\n",
      "\n",
      "Epoch 00118: val_f1_metric did not improve from 0.66536\n",
      "Epoch 119/200\n",
      "30/30 [==============================] - 2s 51ms/step - loss: 0.3062 - accuracy: 0.6951 - f1_metric: 0.6934 - val_loss: 0.6266 - val_accuracy: 0.6463 - val_f1_metric: 0.6451\n",
      "\n",
      "Epoch 00119: val_f1_metric did not improve from 0.66536\n",
      "Epoch 120/200\n",
      "30/30 [==============================] - 2s 53ms/step - loss: 0.3144 - accuracy: 0.7125 - f1_metric: 0.7113 - val_loss: 0.6181 - val_accuracy: 0.6505 - val_f1_metric: 0.6490\n",
      "\n",
      "Epoch 00120: val_f1_metric did not improve from 0.66536\n",
      "Epoch 121/200\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 0.3296 - accuracy: 0.6988 - f1_metric: 0.6990 - val_loss: 0.6430 - val_accuracy: 0.6337 - val_f1_metric: 0.6334\n",
      "\n",
      "Epoch 00121: val_f1_metric did not improve from 0.66536\n",
      "Epoch 122/200\n",
      "30/30 [==============================] - 2s 51ms/step - loss: 0.3171 - accuracy: 0.7014 - f1_metric: 0.7011 - val_loss: 0.6018 - val_accuracy: 0.6505 - val_f1_metric: 0.6490\n",
      "\n",
      "Epoch 00122: val_f1_metric did not improve from 0.66536\n",
      "Epoch 123/200\n",
      "30/30 [==============================] - 2s 62ms/step - loss: 0.3021 - accuracy: 0.7120 - f1_metric: 0.7118 - val_loss: 0.5927 - val_accuracy: 0.6653 - val_f1_metric: 0.6627\n",
      "\n",
      "Epoch 00123: val_f1_metric did not improve from 0.66536\n",
      "Epoch 124/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 0.3083 - accuracy: 0.7125 - f1_metric: 0.7110 - val_loss: 0.6058 - val_accuracy: 0.6611 - val_f1_metric: 0.6615\n",
      "\n",
      "Epoch 00124: val_f1_metric did not improve from 0.66536\n",
      "Epoch 125/200\n",
      "30/30 [==============================] - 2s 50ms/step - loss: 0.3213 - accuracy: 0.7077 - f1_metric: 0.7064 - val_loss: 0.6169 - val_accuracy: 0.6421 - val_f1_metric: 0.6412\n",
      "\n",
      "Epoch 00125: val_f1_metric did not improve from 0.66536\n",
      "Epoch 126/200\n",
      "30/30 [==============================] - 2s 50ms/step - loss: 0.3111 - accuracy: 0.7130 - f1_metric: 0.7131 - val_loss: 0.6007 - val_accuracy: 0.6505 - val_f1_metric: 0.6490\n",
      "\n",
      "Epoch 00126: val_f1_metric did not improve from 0.66536\n",
      "Epoch 127/200\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.3020 - accuracy: 0.7083 - f1_metric: 0.7079 - val_loss: 0.6008 - val_accuracy: 0.6632 - val_f1_metric: 0.6607\n",
      "\n",
      "Epoch 00127: val_f1_metric did not improve from 0.66536\n",
      "Epoch 128/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 2s 55ms/step - loss: 0.3176 - accuracy: 0.7172 - f1_metric: 0.7168 - val_loss: 0.6102 - val_accuracy: 0.6611 - val_f1_metric: 0.6615\n",
      "\n",
      "Epoch 00128: val_f1_metric did not improve from 0.66536\n",
      "Epoch 129/200\n",
      "30/30 [==============================] - 1s 50ms/step - loss: 0.3107 - accuracy: 0.7062 - f1_metric: 0.7051 - val_loss: 0.6221 - val_accuracy: 0.6463 - val_f1_metric: 0.6478\n",
      "\n",
      "Epoch 00129: val_f1_metric did not improve from 0.66536\n",
      "Epoch 130/200\n",
      "30/30 [==============================] - 2s 52ms/step - loss: 0.3017 - accuracy: 0.7330 - f1_metric: 0.7329 - val_loss: 0.6010 - val_accuracy: 0.6589 - val_f1_metric: 0.6595\n",
      "\n",
      "Epoch 00130: val_f1_metric did not improve from 0.66536\n",
      "Epoch 131/200\n",
      "30/30 [==============================] - 2s 51ms/step - loss: 0.3385 - accuracy: 0.7051 - f1_metric: 0.7066 - val_loss: 0.6513 - val_accuracy: 0.6253 - val_f1_metric: 0.6256\n",
      "\n",
      "Epoch 00131: val_f1_metric did not improve from 0.66536\n",
      "Epoch 132/200\n",
      "30/30 [==============================] - 2s 52ms/step - loss: 0.3105 - accuracy: 0.7083 - f1_metric: 0.7087 - val_loss: 0.5880 - val_accuracy: 0.6695 - val_f1_metric: 0.6719\n",
      "\n",
      "Epoch 00132: val_f1_metric improved from 0.66536 to 0.67195, saving model to ./best_model_keras\n",
      "INFO:tensorflow:Assets written to: ./best_model_keras/assets\n",
      "Epoch 133/200\n",
      "30/30 [==============================] - 2s 70ms/step - loss: 0.3278 - accuracy: 0.6962 - f1_metric: 0.6974 - val_loss: 0.6164 - val_accuracy: 0.6484 - val_f1_metric: 0.6471\n",
      "\n",
      "Epoch 00133: val_f1_metric did not improve from 0.67195\n",
      "Epoch 134/200\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 0.3042 - accuracy: 0.7156 - f1_metric: 0.7154 - val_loss: 0.6020 - val_accuracy: 0.6611 - val_f1_metric: 0.6615\n",
      "\n",
      "Epoch 00134: val_f1_metric did not improve from 0.67195\n",
      "Epoch 135/200\n",
      "30/30 [==============================] - 2s 60ms/step - loss: 0.3080 - accuracy: 0.7225 - f1_metric: 0.7220 - val_loss: 0.6043 - val_accuracy: 0.6632 - val_f1_metric: 0.6634\n",
      "\n",
      "Epoch 00135: val_f1_metric did not improve from 0.67195\n",
      "Epoch 136/200\n",
      "30/30 [==============================] - 2s 59ms/step - loss: 0.3047 - accuracy: 0.7141 - f1_metric: 0.7139 - val_loss: 0.5938 - val_accuracy: 0.6737 - val_f1_metric: 0.6785\n",
      "\n",
      "Epoch 00136: val_f1_metric improved from 0.67195 to 0.67853, saving model to ./best_model_keras\n",
      "INFO:tensorflow:Assets written to: ./best_model_keras/assets\n",
      "Epoch 137/200\n",
      "30/30 [==============================] - 2s 58ms/step - loss: 0.3261 - accuracy: 0.7114 - f1_metric: 0.7110 - val_loss: 0.6215 - val_accuracy: 0.6505 - val_f1_metric: 0.6517\n",
      "\n",
      "Epoch 00137: val_f1_metric did not improve from 0.67853\n",
      "Epoch 138/200\n",
      "30/30 [==============================] - 2s 58ms/step - loss: 0.3104 - accuracy: 0.7162 - f1_metric: 0.7161 - val_loss: 0.5864 - val_accuracy: 0.6716 - val_f1_metric: 0.6712\n",
      "\n",
      "Epoch 00138: val_f1_metric did not improve from 0.67853\n",
      "Epoch 139/200\n",
      "30/30 [==============================] - 2s 62ms/step - loss: 0.3095 - accuracy: 0.7278 - f1_metric: 0.7272 - val_loss: 0.6126 - val_accuracy: 0.6547 - val_f1_metric: 0.6556\n",
      "\n",
      "Epoch 00139: val_f1_metric did not improve from 0.67853\n",
      "Epoch 140/200\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 0.3071 - accuracy: 0.7204 - f1_metric: 0.7201 - val_loss: 0.6197 - val_accuracy: 0.6632 - val_f1_metric: 0.6607\n",
      "\n",
      "Epoch 00140: val_f1_metric did not improve from 0.67853\n",
      "Epoch 141/200\n",
      "30/30 [==============================] - 2s 66ms/step - loss: 0.3016 - accuracy: 0.7062 - f1_metric: 0.7061 - val_loss: 0.6005 - val_accuracy: 0.6716 - val_f1_metric: 0.6739\n",
      "\n",
      "Epoch 00141: val_f1_metric did not improve from 0.67853\n",
      "Epoch 142/200\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 0.3068 - accuracy: 0.7251 - f1_metric: 0.7243 - val_loss: 0.6128 - val_accuracy: 0.6589 - val_f1_metric: 0.6622\n",
      "\n",
      "Epoch 00142: val_f1_metric did not improve from 0.67853\n",
      "Epoch 143/200\n",
      "30/30 [==============================] - 2s 60ms/step - loss: 0.3051 - accuracy: 0.7104 - f1_metric: 0.7092 - val_loss: 0.6196 - val_accuracy: 0.6568 - val_f1_metric: 0.6602\n",
      "\n",
      "Epoch 00143: val_f1_metric did not improve from 0.67853\n",
      "Epoch 144/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 0.2981 - accuracy: 0.7320 - f1_metric: 0.7329 - val_loss: 0.6011 - val_accuracy: 0.6716 - val_f1_metric: 0.6739\n",
      "\n",
      "Epoch 00144: val_f1_metric did not improve from 0.67853\n",
      "Epoch 145/200\n",
      "30/30 [==============================] - 2s 58ms/step - loss: 0.3027 - accuracy: 0.7172 - f1_metric: 0.7165 - val_loss: 0.6048 - val_accuracy: 0.6737 - val_f1_metric: 0.6759\n",
      "\n",
      "Epoch 00145: val_f1_metric did not improve from 0.67853\n",
      "Epoch 146/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 0.3076 - accuracy: 0.7162 - f1_metric: 0.7165 - val_loss: 0.5993 - val_accuracy: 0.6653 - val_f1_metric: 0.6654\n",
      "\n",
      "Epoch 00146: val_f1_metric did not improve from 0.67853\n",
      "Epoch 147/200\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 0.3027 - accuracy: 0.7120 - f1_metric: 0.7121 - val_loss: 0.6152 - val_accuracy: 0.6568 - val_f1_metric: 0.6576\n",
      "\n",
      "Epoch 00147: val_f1_metric did not improve from 0.67853\n",
      "Epoch 148/200\n",
      "30/30 [==============================] - 2s 62ms/step - loss: 0.2920 - accuracy: 0.7357 - f1_metric: 0.7373 - val_loss: 0.5658 - val_accuracy: 0.6821 - val_f1_metric: 0.6890\n",
      "\n",
      "Epoch 00148: val_f1_metric improved from 0.67853 to 0.68902, saving model to ./best_model_keras\n",
      "INFO:tensorflow:Assets written to: ./best_model_keras/assets\n",
      "Epoch 149/200\n",
      "30/30 [==============================] - 2s 64ms/step - loss: 0.3128 - accuracy: 0.7288 - f1_metric: 0.7282 - val_loss: 0.6299 - val_accuracy: 0.6568 - val_f1_metric: 0.6602\n",
      "\n",
      "Epoch 00149: val_f1_metric did not improve from 0.68902\n",
      "Epoch 150/200\n",
      "30/30 [==============================] - 2s 64ms/step - loss: 0.3103 - accuracy: 0.7067 - f1_metric: 0.7071 - val_loss: 0.6202 - val_accuracy: 0.6611 - val_f1_metric: 0.6615\n",
      "\n",
      "Epoch 00150: val_f1_metric did not improve from 0.68902\n",
      "Epoch 151/200\n",
      "30/30 [==============================] - 2s 60ms/step - loss: 0.2946 - accuracy: 0.7272 - f1_metric: 0.7259 - val_loss: 0.6109 - val_accuracy: 0.6568 - val_f1_metric: 0.6576\n",
      "\n",
      "Epoch 00151: val_f1_metric did not improve from 0.68902\n",
      "Epoch 152/200\n",
      "30/30 [==============================] - 2s 60ms/step - loss: 0.2958 - accuracy: 0.7199 - f1_metric: 0.7204 - val_loss: 0.5884 - val_accuracy: 0.6779 - val_f1_metric: 0.6824\n",
      "\n",
      "Epoch 00152: val_f1_metric did not improve from 0.68902\n",
      "Epoch 153/200\n",
      "30/30 [==============================] - 2s 58ms/step - loss: 0.3041 - accuracy: 0.7299 - f1_metric: 0.7308 - val_loss: 0.6207 - val_accuracy: 0.6611 - val_f1_metric: 0.6615\n",
      "\n",
      "Epoch 00153: val_f1_metric did not improve from 0.68902\n",
      "Epoch 154/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 0.2858 - accuracy: 0.7267 - f1_metric: 0.7269 - val_loss: 0.5924 - val_accuracy: 0.6737 - val_f1_metric: 0.6759\n",
      "\n",
      "Epoch 00154: val_f1_metric did not improve from 0.68902\n",
      "Epoch 155/200\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 0.3009 - accuracy: 0.7309 - f1_metric: 0.7305 - val_loss: 0.6036 - val_accuracy: 0.6653 - val_f1_metric: 0.6654\n",
      "\n",
      "Epoch 00155: val_f1_metric did not improve from 0.68902\n",
      "Epoch 156/200\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 0.3137 - accuracy: 0.7193 - f1_metric: 0.7178 - val_loss: 0.6244 - val_accuracy: 0.6526 - val_f1_metric: 0.6536\n",
      "\n",
      "Epoch 00156: val_f1_metric did not improve from 0.68902\n",
      "Epoch 157/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 0.2884 - accuracy: 0.7251 - f1_metric: 0.7256 - val_loss: 0.5835 - val_accuracy: 0.6800 - val_f1_metric: 0.6844\n",
      "\n",
      "Epoch 00157: val_f1_metric did not improve from 0.68902\n",
      "Epoch 158/200\n",
      "30/30 [==============================] - 2s 51ms/step - loss: 0.3086 - accuracy: 0.7172 - f1_metric: 0.7178 - val_loss: 0.6100 - val_accuracy: 0.6632 - val_f1_metric: 0.6661\n",
      "\n",
      "Epoch 00158: val_f1_metric did not improve from 0.68902\n",
      "Epoch 159/200\n",
      "30/30 [==============================] - 2s 62ms/step - loss: 0.3093 - accuracy: 0.7130 - f1_metric: 0.7131 - val_loss: 0.6192 - val_accuracy: 0.6526 - val_f1_metric: 0.6563\n",
      "\n",
      "Epoch 00159: val_f1_metric did not improve from 0.68902\n",
      "Epoch 160/200\n",
      "30/30 [==============================] - 2s 50ms/step - loss: 0.2788 - accuracy: 0.7346 - f1_metric: 0.7342 - val_loss: 0.5860 - val_accuracy: 0.6842 - val_f1_metric: 0.6910\n",
      "\n",
      "Epoch 00160: val_f1_metric improved from 0.68902 to 0.69097, saving model to ./best_model_keras\n",
      "INFO:tensorflow:Assets written to: ./best_model_keras/assets\n",
      "Epoch 161/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 0.2981 - accuracy: 0.7467 - f1_metric: 0.7462 - val_loss: 0.5969 - val_accuracy: 0.6779 - val_f1_metric: 0.6798\n",
      "\n",
      "Epoch 00161: val_f1_metric did not improve from 0.69097\n",
      "Epoch 162/200\n",
      "30/30 [==============================] - 2s 62ms/step - loss: 0.2992 - accuracy: 0.7320 - f1_metric: 0.7321 - val_loss: 0.6071 - val_accuracy: 0.6716 - val_f1_metric: 0.6739\n",
      "\n",
      "Epoch 00162: val_f1_metric did not improve from 0.69097\n",
      "Epoch 163/200\n",
      "30/30 [==============================] - 2s 52ms/step - loss: 0.3006 - accuracy: 0.7288 - f1_metric: 0.7292 - val_loss: 0.6308 - val_accuracy: 0.6568 - val_f1_metric: 0.6602\n",
      "\n",
      "Epoch 00163: val_f1_metric did not improve from 0.69097\n",
      "Epoch 164/200\n",
      "30/30 [==============================] - 2s 59ms/step - loss: 0.2967 - accuracy: 0.7183 - f1_metric: 0.7191 - val_loss: 0.6124 - val_accuracy: 0.6547 - val_f1_metric: 0.6583\n",
      "\n",
      "Epoch 00164: val_f1_metric did not improve from 0.69097\n",
      "Epoch 165/200\n",
      "30/30 [==============================] - 2s 61ms/step - loss: 0.2915 - accuracy: 0.7267 - f1_metric: 0.7264 - val_loss: 0.6146 - val_accuracy: 0.6547 - val_f1_metric: 0.6583\n",
      "\n",
      "Epoch 00165: val_f1_metric did not improve from 0.69097\n",
      "Epoch 166/200\n",
      "30/30 [==============================] - 2s 62ms/step - loss: 0.2912 - accuracy: 0.7335 - f1_metric: 0.7339 - val_loss: 0.6004 - val_accuracy: 0.6716 - val_f1_metric: 0.6739\n",
      "\n",
      "Epoch 00166: val_f1_metric did not improve from 0.69097\n",
      "Epoch 167/200\n",
      "30/30 [==============================] - 2s 63ms/step - loss: 0.3031 - accuracy: 0.7209 - f1_metric: 0.7209 - val_loss: 0.6071 - val_accuracy: 0.6674 - val_f1_metric: 0.6727\n",
      "\n",
      "Epoch 00167: val_f1_metric did not improve from 0.69097\n",
      "Epoch 168/200\n",
      "30/30 [==============================] - 2s 63ms/step - loss: 0.2886 - accuracy: 0.7320 - f1_metric: 0.7316 - val_loss: 0.5664 - val_accuracy: 0.6905 - val_f1_metric: 0.6942\n",
      "\n",
      "Epoch 00168: val_f1_metric improved from 0.69097 to 0.69416, saving model to ./best_model_keras\n",
      "INFO:tensorflow:Assets written to: ./best_model_keras/assets\n",
      "Epoch 169/200\n",
      "30/30 [==============================] - 2s 70ms/step - loss: 0.2759 - accuracy: 0.7446 - f1_metric: 0.7449 - val_loss: 0.5975 - val_accuracy: 0.6737 - val_f1_metric: 0.6759\n",
      "\n",
      "Epoch 00169: val_f1_metric did not improve from 0.69416\n",
      "Epoch 170/200\n",
      "30/30 [==============================] - 2s 61ms/step - loss: 0.2791 - accuracy: 0.7583 - f1_metric: 0.7574 - val_loss: 0.5743 - val_accuracy: 0.6842 - val_f1_metric: 0.6883\n",
      "\n",
      "Epoch 00170: val_f1_metric did not improve from 0.69416\n",
      "Epoch 171/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 0.2849 - accuracy: 0.7430 - f1_metric: 0.7433 - val_loss: 0.5923 - val_accuracy: 0.6695 - val_f1_metric: 0.6719\n",
      "\n",
      "Epoch 00171: val_f1_metric did not improve from 0.69416\n",
      "Epoch 172/200\n",
      "30/30 [==============================] - 2s 65ms/step - loss: 0.2935 - accuracy: 0.7251 - f1_metric: 0.7258 - val_loss: 0.5874 - val_accuracy: 0.6779 - val_f1_metric: 0.6798\n",
      "\n",
      "Epoch 00172: val_f1_metric did not improve from 0.69416\n",
      "Epoch 173/200\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 0.2889 - accuracy: 0.7388 - f1_metric: 0.7391 - val_loss: 0.5761 - val_accuracy: 0.6842 - val_f1_metric: 0.6856\n",
      "\n",
      "Epoch 00173: val_f1_metric did not improve from 0.69416\n",
      "Epoch 174/200\n",
      "30/30 [==============================] - 2s 58ms/step - loss: 0.2965 - accuracy: 0.7304 - f1_metric: 0.7303 - val_loss: 0.5960 - val_accuracy: 0.6674 - val_f1_metric: 0.6700\n",
      "\n",
      "Epoch 00174: val_f1_metric did not improve from 0.69416\n",
      "Epoch 175/200\n",
      "30/30 [==============================] - 2s 59ms/step - loss: 0.2872 - accuracy: 0.7404 - f1_metric: 0.7399 - val_loss: 0.5941 - val_accuracy: 0.6716 - val_f1_metric: 0.6739\n",
      "\n",
      "Epoch 00175: val_f1_metric did not improve from 0.69416\n",
      "Epoch 176/200\n",
      "30/30 [==============================] - 2s 52ms/step - loss: 0.2786 - accuracy: 0.7314 - f1_metric: 0.7326 - val_loss: 0.5903 - val_accuracy: 0.6779 - val_f1_metric: 0.6798\n",
      "\n",
      "Epoch 00176: val_f1_metric did not improve from 0.69416\n",
      "Epoch 177/200\n",
      "30/30 [==============================] - 2s 58ms/step - loss: 0.2897 - accuracy: 0.7304 - f1_metric: 0.7313 - val_loss: 0.6067 - val_accuracy: 0.6653 - val_f1_metric: 0.6680\n",
      "\n",
      "Epoch 00177: val_f1_metric did not improve from 0.69416\n",
      "Epoch 178/200\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 0.2723 - accuracy: 0.7551 - f1_metric: 0.7550 - val_loss: 0.5983 - val_accuracy: 0.6800 - val_f1_metric: 0.6817\n",
      "\n",
      "Epoch 00178: val_f1_metric did not improve from 0.69416\n",
      "Epoch 179/200\n",
      "30/30 [==============================] - 1s 49ms/step - loss: 0.2977 - accuracy: 0.7351 - f1_metric: 0.7347 - val_loss: 0.6164 - val_accuracy: 0.6611 - val_f1_metric: 0.6641\n",
      "\n",
      "Epoch 00179: val_f1_metric did not improve from 0.69416\n",
      "Epoch 180/200\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 0.2906 - accuracy: 0.7451 - f1_metric: 0.7449 - val_loss: 0.6074 - val_accuracy: 0.6674 - val_f1_metric: 0.6673\n",
      "\n",
      "Epoch 00180: val_f1_metric did not improve from 0.69416\n",
      "Epoch 181/200\n",
      "30/30 [==============================] - 2s 60ms/step - loss: 0.2814 - accuracy: 0.7357 - f1_metric: 0.7347 - val_loss: 0.6125 - val_accuracy: 0.6674 - val_f1_metric: 0.6700\n",
      "\n",
      "Epoch 00181: val_f1_metric did not improve from 0.69416\n",
      "Epoch 182/200\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 0.2751 - accuracy: 0.7451 - f1_metric: 0.7456 - val_loss: 0.5627 - val_accuracy: 0.6947 - val_f1_metric: 0.7007\n",
      "\n",
      "Epoch 00182: val_f1_metric improved from 0.69416 to 0.70074, saving model to ./best_model_keras\n",
      "INFO:tensorflow:Assets written to: ./best_model_keras/assets\n",
      "Epoch 183/200\n",
      "30/30 [==============================] - 2s 66ms/step - loss: 0.2836 - accuracy: 0.7320 - f1_metric: 0.7311 - val_loss: 0.6020 - val_accuracy: 0.6758 - val_f1_metric: 0.6778\n",
      "\n",
      "Epoch 00183: val_f1_metric did not improve from 0.70074\n",
      "Epoch 184/200\n",
      "30/30 [==============================] - 2s 60ms/step - loss: 0.2774 - accuracy: 0.7462 - f1_metric: 0.7467 - val_loss: 0.5720 - val_accuracy: 0.6884 - val_f1_metric: 0.6868\n",
      "\n",
      "Epoch 00184: val_f1_metric did not improve from 0.70074\n",
      "Epoch 185/200\n",
      "30/30 [==============================] - 2s 59ms/step - loss: 0.2711 - accuracy: 0.7520 - f1_metric: 0.7516 - val_loss: 0.5784 - val_accuracy: 0.6947 - val_f1_metric: 0.6981\n",
      "\n",
      "Epoch 00185: val_f1_metric did not improve from 0.70074\n",
      "Epoch 186/200\n",
      "30/30 [==============================] - 2s 58ms/step - loss: 0.2943 - accuracy: 0.7399 - f1_metric: 0.7397 - val_loss: 0.6347 - val_accuracy: 0.6568 - val_f1_metric: 0.6602\n",
      "\n",
      "Epoch 00186: val_f1_metric did not improve from 0.70074\n",
      "Epoch 187/200\n",
      "30/30 [==============================] - 2s 59ms/step - loss: 0.2844 - accuracy: 0.7383 - f1_metric: 0.7391 - val_loss: 0.6005 - val_accuracy: 0.6632 - val_f1_metric: 0.6661\n",
      "\n",
      "Epoch 00187: val_f1_metric did not improve from 0.70074\n",
      "Epoch 188/200\n",
      "30/30 [==============================] - 2s 52ms/step - loss: 0.2714 - accuracy: 0.7557 - f1_metric: 0.7563 - val_loss: 0.5740 - val_accuracy: 0.6947 - val_f1_metric: 0.6981\n",
      "\n",
      "Epoch 00188: val_f1_metric did not improve from 0.70074\n",
      "Epoch 189/200\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 0.2903 - accuracy: 0.7372 - f1_metric: 0.7376 - val_loss: 0.6239 - val_accuracy: 0.6589 - val_f1_metric: 0.6649\n",
      "\n",
      "Epoch 00189: val_f1_metric did not improve from 0.70074\n",
      "Epoch 190/200\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 0.2916 - accuracy: 0.7430 - f1_metric: 0.7441 - val_loss: 0.6078 - val_accuracy: 0.6695 - val_f1_metric: 0.6693\n",
      "\n",
      "Epoch 00190: val_f1_metric did not improve from 0.70074\n",
      "Epoch 191/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 0.2795 - accuracy: 0.7446 - f1_metric: 0.7446 - val_loss: 0.5826 - val_accuracy: 0.6926 - val_f1_metric: 0.6988\n",
      "\n",
      "Epoch 00191: val_f1_metric did not improve from 0.70074\n",
      "Epoch 192/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 2s 51ms/step - loss: 0.2980 - accuracy: 0.7251 - f1_metric: 0.7261 - val_loss: 0.6082 - val_accuracy: 0.6737 - val_f1_metric: 0.6785\n",
      "\n",
      "Epoch 00192: val_f1_metric did not improve from 0.70074\n",
      "Epoch 193/200\n",
      "30/30 [==============================] - 1s 49ms/step - loss: 0.2847 - accuracy: 0.7457 - f1_metric: 0.7467 - val_loss: 0.5815 - val_accuracy: 0.6905 - val_f1_metric: 0.6915\n",
      "\n",
      "Epoch 00193: val_f1_metric did not improve from 0.70074\n",
      "Epoch 194/200\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 0.2779 - accuracy: 0.7557 - f1_metric: 0.7558 - val_loss: 0.6100 - val_accuracy: 0.6674 - val_f1_metric: 0.6700\n",
      "\n",
      "Epoch 00194: val_f1_metric did not improve from 0.70074\n",
      "Epoch 195/200\n",
      "30/30 [==============================] - 2s 50ms/step - loss: 0.2728 - accuracy: 0.7615 - f1_metric: 0.7623 - val_loss: 0.5957 - val_accuracy: 0.6821 - val_f1_metric: 0.6863\n",
      "\n",
      "Epoch 00195: val_f1_metric did not improve from 0.70074\n",
      "Epoch 196/200\n",
      "30/30 [==============================] - 2s 52ms/step - loss: 0.2853 - accuracy: 0.7541 - f1_metric: 0.7542 - val_loss: 0.5925 - val_accuracy: 0.6926 - val_f1_metric: 0.7015\n",
      "\n",
      "Epoch 00196: val_f1_metric improved from 0.70074 to 0.70146, saving model to ./best_model_keras\n",
      "INFO:tensorflow:Assets written to: ./best_model_keras/assets\n",
      "Epoch 197/200\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 0.2760 - accuracy: 0.7430 - f1_metric: 0.7433 - val_loss: 0.6085 - val_accuracy: 0.6632 - val_f1_metric: 0.6634\n",
      "\n",
      "Epoch 00197: val_f1_metric did not improve from 0.70146\n",
      "Epoch 198/200\n",
      "30/30 [==============================] - 1s 49ms/step - loss: 0.2788 - accuracy: 0.7357 - f1_metric: 0.7357 - val_loss: 0.5707 - val_accuracy: 0.6989 - val_f1_metric: 0.7020\n",
      "\n",
      "Epoch 00198: val_f1_metric improved from 0.70146 to 0.70197, saving model to ./best_model_keras\n",
      "INFO:tensorflow:Assets written to: ./best_model_keras/assets\n",
      "Epoch 199/200\n",
      "30/30 [==============================] - 2s 52ms/step - loss: 0.2765 - accuracy: 0.7551 - f1_metric: 0.7555 - val_loss: 0.6132 - val_accuracy: 0.6821 - val_f1_metric: 0.6863\n",
      "\n",
      "Epoch 00199: val_f1_metric did not improve from 0.70197\n",
      "Epoch 200/200\n",
      "30/30 [==============================] - 2s 53ms/step - loss: 0.2777 - accuracy: 0.7593 - f1_metric: 0.7594 - val_loss: 0.5946 - val_accuracy: 0.6821 - val_f1_metric: 0.6810\n",
      "\n",
      "Epoch 00200: val_f1_metric did not improve from 0.70197\n",
      "CPU times: user 11min 10s, sys: 1min 30s, total: 12min 41s\n",
      "Wall time: 6min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = model.fit(x_train, y_train, epochs=params['epochs'], verbose=1,\n",
    "                            batch_size=64, shuffle=True,\n",
    "                            # validation_split=0.3,\n",
    "                            validation_data=(x_cv, y_cv),\n",
    "                            callbacks=[mcp, rlp, es]\n",
    "                            , sample_weight=sample_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "#InteractiveShell.ast_node_interactivity = \"last\"\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot(history.history['f1_metric'])\n",
    "plt.plot(history.history['val_f1_metric'])\n",
    "\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train_loss', 'val_loss', 'f1', 'val_f1'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras evaluate= [0.752422571182251, 0.6228955984115601, 0.6226242184638977]\n",
      "size of test set 594\n",
      "TP class counts (array([0, 1, 2]), array([ 21,  21, 328]))\n",
      "True class counts (array([0, 1, 2]), array([ 34,  33, 527]))\n",
      "Pred class counts (array([0, 1, 2]), array([133, 108, 353]))\n",
      "baseline acc: 88.72053872053873\n",
      "[[ 21   0  13]\n",
      " [  0  21  12]\n",
      " [112  87 328]]\n",
      "F1 score (weighted) 0.6923152035221899\n",
      "F1 score (macro) 0.43160796395603374\n",
      "F1 score (micro) 0.622895622895623\n",
      "cohen's Kappa 0.1616882666851479\n",
      "Recall of class 0 = 0.62\n",
      "Recall of class 1 = 0.64\n",
      "Recall of class 2 = 0.62\n",
      "Recall avg 0.6266666666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_auc_score, cohen_kappa_score\n",
    "import seaborn as sns\n",
    "\n",
    "model = load_model(best_model_path)\n",
    "test_res = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"keras evaluate=\", test_res)\n",
    "pred = model.predict(x_test)\n",
    "pred_classes = np.argmax(pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "check_baseline(pred_classes, y_test_classes)\n",
    "conf_mat = confusion_matrix(y_test_classes, pred_classes)\n",
    "print(conf_mat)\n",
    "labels = [0,1,2]\n",
    "\n",
    "f1_weighted = f1_score(y_test_classes, pred_classes, labels=None, \n",
    "         average='weighted', sample_weight=None)\n",
    "print(\"F1 score (weighted)\", f1_weighted)\n",
    "print(\"F1 score (macro)\", f1_score(y_test_classes, pred_classes, labels=None, \n",
    "         average='macro', sample_weight=None))\n",
    "print(\"F1 score (micro)\", f1_score(y_test_classes, pred_classes, labels=None, \n",
    "         average='micro', sample_weight=None))  # weighted and micro preferred in case of imbalance\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/model_evaluation.html#cohen-s-kappa --> supports multiclass; ref: https://stats.stackexchange.com/questions/82162/cohens-kappa-in-plain-english\n",
    "print(\"cohen's Kappa\", cohen_kappa_score(y_test_classes, pred_classes))\n",
    "\n",
    "recall = []\n",
    "for i, row in enumerate(conf_mat):\n",
    "    recall.append(np.round(row[i]/np.sum(row), 2))\n",
    "    print(\"Recall of class {} = {}\".format(i, recall[i]))\n",
    "print(\"Recall avg\", sum(recall)/len(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 2 2 0 2 2 2 1 2 2 2 2 2 0 0 2 2 1 2 0 2 1 0 0 0 0 2 0 1 1 2 2 2 2 0 1\n",
      " 2 2 2 0 1 2 2 2 0 2 2 1 2 2 2 2 2 0 2 0 0 0 2 2 2 2 2 2 0 2 1 1 2 2 0 2 0\n",
      " 0 0 2 2 2 1 2 1 2 1 2 2 2 0 2 2 0 0 1 2 1 1 2 2 0 2 2 0 1 0 1 0 0 2 1 2 2\n",
      " 2 0 2 2 2 2 0 2 2 1 1 2 2 2 2 2 2 2 2 0 2 0 2 2 0 0 0 1 2 2 0 2 0 2 2 2 1\n",
      " 1 2 2 0 2 1 0 1 1 2 2 0 2 2 2 1 0 0 0 2 0 2 0 0 2 2 2 2 1 2 2 0 2 0 2 1 1\n",
      " 2 2 2 2 2 2 1 0 2 2 0 2 1 1 1 2 2 2 1 2 1 0 1 0 0 2 2 2 1 2 1 2 2 2 0 1 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 0 0 2 2 2 2 1 1 2 2 0 2 0 2 2 2 0 2 2 2 2 2 2 1 0\n",
      " 2 0 2 2 0 2 1 2 0 2 0 0 2 2 2 0 2 2 2 0 1 2 2 2 1 2 1 2 2 2 1 2 2 2 2 2 2\n",
      " 0 0 2 0 0 2 2 0 0 2 2 2 1 2 2 2 2 1 1 2 2 2 2 1 1 0 1 1 1 2 0 2 2 2 1 2 1\n",
      " 1 2 1 2 2 1 0 2 1 2 0 0 2 2 2 0 0 2 2 2 2 2 2 1 2 2 2 2 1 2 2 2 2 2 1 2 2\n",
      " 2 0 0 0 2 0 1 0 1 2 2 0 2 2 0 0 2 1 0 2 2 2 2 2 2 2 1 2 0 2 1 0 2 1 2 2 2\n",
      " 1 2 0 1 2 2 2 2 2 2 2 1 2 0 2 2 0 1 0 2 2 2 0 2 2 2 2 0 0 2 2 1 2 2 1 2 2\n",
      " 2 2 2 2 2 1 0 2 0 1 2 0 1 2 1 2 2 0 2 1 2 2 2 2 1 1 1 0 2 2 2 2 2 1 2 1 2\n",
      " 0 1 0 2 2 0 0 2 2 1 2 2 2 2 2 0 2 2 2 2 2 1 2 0 2 2 2 2 0 1 2 2 0 1 2 2 0\n",
      " 1 1 1 0 2 2 2 1 0 2 2 2 0 1 2 2 0 0 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 0 2 0\n",
      " 2 2 0 0 0 2 2 0 0 0 1 1 2 0 1 2 0 2 1 2 2 1 2 0 2 1 2 1 2 1 2 2 0 1 2 0 2\n",
      " 0 2]\n"
     ]
    }
   ],
   "source": [
    "print(pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
